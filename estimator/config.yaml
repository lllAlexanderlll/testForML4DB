# Neural Network
width: 64
layer:
  - 64
  - 32
loss_function: mean_squared_error
dropout: 0.2
learning_rate: 0.0001
kernel_initializer: normal
activation_strategy: relu
len_input: