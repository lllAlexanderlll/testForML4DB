{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Local Cardinality Estimation KP SS2020 The submodule 'meta-collector' collects the several informations from the requested table and saves the information into a .json file The submodule 'sql-generator' uses the output of the meta-collector to create random SQL-Queries with the corresponding schema The submodule 'vectorizer' uses the output of the sql-generator to encode it into a vectors The submodule 'estimator' takes the encoded vectors and uses them on a neural network The submodule 'postrgres-evaluator' takes the sql-queries and executes them on the postgres-database to get the true cardinality For building the Documentation you need to execute the setup_doc.sh . This script installs the prerequisites if not already installed, builds the documentation and starts the documentation-server.","title":"Home"},{"location":"#local-cardinality-estimation","text":"KP SS2020 The submodule 'meta-collector' collects the several informations from the requested table and saves the information into a .json file The submodule 'sql-generator' uses the output of the meta-collector to create random SQL-Queries with the corresponding schema The submodule 'vectorizer' uses the output of the sql-generator to encode it into a vectors The submodule 'estimator' takes the encoded vectors and uses them on a neural network The submodule 'postrgres-evaluator' takes the sql-queries and executes them on the postgres-database to get the true cardinality For building the Documentation you need to execute the setup_doc.sh . This script installs the prerequisites if not already installed, builds the documentation and starts the documentation-server.","title":"Local Cardinality Estimation"},{"location":"databaseconnector/databaseconnector-class/","text":"database_connector.database_connector Database Objects class Database(Enum) Enum for the different supported databases. If you use MySQL, then use MARIADB as value here. DatabaseConnector Objects class DatabaseConnector() Class for DatabaseConnector. __init__ | __init__(database: Database, debug: bool = True) Initializer for the DatabaseConnector Arguments : debug : boolean whether to print additional information while processing connect | connect(config: Dict = None, config_file_path: str = None, sqlite_file_path: str = None) Wrapper method for connecting to the selected database. Arguments : config : if given: It has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 5432 if not given) for PostgreSQL or it has to be a dictionary with at least database, user and password and optionally host and port (default to host: localhost, port: 3306 if not given) for MariaDB. if not given: The config file path is needed and used for these settings. config_file_path : Path to the config file for PostgreSQL or MariaDB. sqlite_file_path : Path to the SQLite database file. connect_to_postgres | connect_to_postgres(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : config : if given: it has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 5432 if not given) if not given: the config file 'config.yaml' is used for these settings config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a .yaml/.yml file connect_to_mariadb | connect_to_mariadb(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : config : if given: it has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 3306 if not given) if not given: the config file 'config.yaml' is used for these settings config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a .yaml/.yml file connect_to_sqlite | connect_to_sqlite(database_file_path: str) Open connection to a sqlite database. Arguments : database_file_path : The path to the sqlite database file. close_database_connection | close_database_connection() close the connection to the database Returns : void execute | execute(sql_string: str) Method for executing a SQL-Query. Arguments : sql_string : The SQL-Query to execute fetchall | fetchall() Wrapper for fetchall method. fetchone | fetchone() Wrapper for fetchone method.","title":"DatabaseConnector Class"},{"location":"databaseconnector/databaseconnector-class/#database_connectordatabase_connector","text":"","title":"database_connector.database_connector"},{"location":"databaseconnector/databaseconnector-class/#database-objects","text":"class Database(Enum) Enum for the different supported databases. If you use MySQL, then use MARIADB as value here.","title":"Database Objects"},{"location":"databaseconnector/databaseconnector-class/#databaseconnector-objects","text":"class DatabaseConnector() Class for DatabaseConnector.","title":"DatabaseConnector Objects"},{"location":"databaseconnector/databaseconnector-class/#__init__","text":"| __init__(database: Database, debug: bool = True) Initializer for the DatabaseConnector Arguments : debug : boolean whether to print additional information while processing","title":"__init__"},{"location":"databaseconnector/databaseconnector-class/#connect","text":"| connect(config: Dict = None, config_file_path: str = None, sqlite_file_path: str = None) Wrapper method for connecting to the selected database. Arguments : config : if given: It has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 5432 if not given) for PostgreSQL or it has to be a dictionary with at least database, user and password and optionally host and port (default to host: localhost, port: 3306 if not given) for MariaDB. if not given: The config file path is needed and used for these settings. config_file_path : Path to the config file for PostgreSQL or MariaDB. sqlite_file_path : Path to the SQLite database file.","title":"connect"},{"location":"databaseconnector/databaseconnector-class/#connect_to_postgres","text":"| connect_to_postgres(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : config : if given: it has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 5432 if not given) if not given: the config file 'config.yaml' is used for these settings config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a .yaml/.yml file","title":"connect_to_postgres"},{"location":"databaseconnector/databaseconnector-class/#connect_to_mariadb","text":"| connect_to_mariadb(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : config : if given: it has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 3306 if not given) if not given: the config file 'config.yaml' is used for these settings config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a .yaml/.yml file","title":"connect_to_mariadb"},{"location":"databaseconnector/databaseconnector-class/#connect_to_sqlite","text":"| connect_to_sqlite(database_file_path: str) Open connection to a sqlite database. Arguments : database_file_path : The path to the sqlite database file.","title":"connect_to_sqlite"},{"location":"databaseconnector/databaseconnector-class/#close_database_connection","text":"| close_database_connection() close the connection to the database Returns : void","title":"close_database_connection"},{"location":"databaseconnector/databaseconnector-class/#execute","text":"| execute(sql_string: str) Method for executing a SQL-Query. Arguments : sql_string : The SQL-Query to execute","title":"execute"},{"location":"databaseconnector/databaseconnector-class/#fetchall","text":"| fetchall() Wrapper for fetchall method.","title":"fetchall"},{"location":"databaseconnector/databaseconnector-class/#fetchone","text":"| fetchone() Wrapper for fetchone method.","title":"fetchone"},{"location":"databaseconnector/databaseconnector-readme/","text":"","title":"DatabaseConnector Readme"},{"location":"estimator/estimator-class/","text":"estimator.estimator Estimator Objects class Estimator() Class containing the neural network for cardinality estimation. The specifications of the neural network can be changed in 'config.yaml'. __init__ | __init__(config: Dict[str, Any] = None, config_file_path: str = \"config.yaml\", data: np.ndarray = None, model: Model = None, model_path: str = None, debug: bool = True) Initializer for the Estimator. Configuration options for the neural network are optionally passed via a config dict. It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". Arguments : config : Only used if neither a model or a model_path is passed. if given: It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". if not given: the config file 'config.yaml' is used for these settings. config_file_path : path for the config-file -> only necessary if no config is given data : Optional parameter for giving the data for training and testing. If given it has to be a Dict with at least \"x\" and \"y\" and optionally \"postgres_estimate\" as keys. The values have to be numpy.ndarray. For key \"x\" it should be the vectorized queries, for key \"y\" the true cardinalities in the same order and for optional key \"postgres_estimate\" the estimates of the postgres optimizer for the query. model : Option to pass a Model which can be used. model_path : Option to pass a path to a saved model in an .h5 file. debug : Boolean whether to print additional information while processing. get_model | get_model(len_input: int, override: bool = False) -> Model Function for creating the model of the neural network with the information from self.config Arguments : len_input : The size of the input vector. override : Whether an existing model should be overridden. Returns : The model for the neural network with the given properties. load_model | load_model(model_path: str) Method for loading an already existing model wich was saved to file. Arguments : model_path : Path to the file containing the model to load denormalize | @staticmethod | denormalize(y, y_min: float, y_max: float) Arguments : y : tensor filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : tensor with denormalized values denormalize_np | @staticmethod | denormalize_np(y: np.ndarray, y_min: float, y_max: float) -> np.ndarray Arguments : y : numpy-array filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : numpy-array with denormalized values load_data_file | load_data_file(file_path: str, override: bool = False) -> Dict[str, np.ndarray] Method for loading the data from file. Arguments : file_path : Path for the file where the data is stored. Has to be a .csv or .npy file. override : Boolean whether to override already existing data. Returns : The data which is set for the Estimator. set_data | set_data(loaded_data: np.ndarray, override: bool = False) Method for setting data and dependent values like max_card and input_length. Arguments : loaded_data : The data loaded from the file. override : Boolean whether to override already existing data. split_data | split_data(split: float = 0.9) Function to split the data into training- and test-set by a parameterized split value. Arguments : split : Percentage of the data going into training set. (split=0.9 means 90% of data is training set) train | train(epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1) -> Union[History, History] Method for training the before created Model. Arguments : epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per epoch. shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. validation_split : How much of the data should be taken as validation set -> these are taken from the training data, not the test data, and are reselected for every epoch. Returns : Training history as dict. test | test() -> np.ndarray Let the trained neural network predict the test data. Returns : numpy-array containing the normalized predictions of the neural network for the test data predict | predict(data: np.ndarray) -> np.ndarray Let the trained neural network predict the given data. Arguments : data : numpy-array containing at least one vectorized query which should be predicted Returns : numpy-array containing the normalized predictions of the neural network for the given data run | run(data_file_path: str = None, epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1, override_model: bool = False, save_model: bool = True, save_model_file_path: str = \"model\") -> np.ndarray Method for a full run of the Estimator, with training and testing. Arguments : data_file_path : Optional path to saved data file. Only necessary if no data has been set before. epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per epoch. shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. validation_split : How much of the data should be taken as validation set -> these are taken from the training data, not the test data, and are reselected for every epoch. override_model : Whether to override a probably already existing model. save_model : Whether to save the trained model to file. save_model_file_path : When save_model==True this parameter is required to give the path where the model should be saved. Returns : A numpy.ndarray containing the calculated q-error. save_model | save_model(filename: str = \"model\") Method for saving the Model to file. Arguments : filename : Name of the file where the model should be stored. (Without file ending. \".h5\" is added to the filename)","title":"Estimator Class"},{"location":"estimator/estimator-class/#estimatorestimator","text":"","title":"estimator.estimator"},{"location":"estimator/estimator-class/#estimator-objects","text":"class Estimator() Class containing the neural network for cardinality estimation. The specifications of the neural network can be changed in 'config.yaml'.","title":"Estimator Objects"},{"location":"estimator/estimator-class/#__init__","text":"| __init__(config: Dict[str, Any] = None, config_file_path: str = \"config.yaml\", data: np.ndarray = None, model: Model = None, model_path: str = None, debug: bool = True) Initializer for the Estimator. Configuration options for the neural network are optionally passed via a config dict. It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". Arguments : config : Only used if neither a model or a model_path is passed. if given: It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". if not given: the config file 'config.yaml' is used for these settings. config_file_path : path for the config-file -> only necessary if no config is given data : Optional parameter for giving the data for training and testing. If given it has to be a Dict with at least \"x\" and \"y\" and optionally \"postgres_estimate\" as keys. The values have to be numpy.ndarray. For key \"x\" it should be the vectorized queries, for key \"y\" the true cardinalities in the same order and for optional key \"postgres_estimate\" the estimates of the postgres optimizer for the query. model : Option to pass a Model which can be used. model_path : Option to pass a path to a saved model in an .h5 file. debug : Boolean whether to print additional information while processing.","title":"__init__"},{"location":"estimator/estimator-class/#get_model","text":"| get_model(len_input: int, override: bool = False) -> Model Function for creating the model of the neural network with the information from self.config Arguments : len_input : The size of the input vector. override : Whether an existing model should be overridden. Returns : The model for the neural network with the given properties.","title":"get_model"},{"location":"estimator/estimator-class/#load_model","text":"| load_model(model_path: str) Method for loading an already existing model wich was saved to file. Arguments : model_path : Path to the file containing the model to load","title":"load_model"},{"location":"estimator/estimator-class/#denormalize","text":"| @staticmethod | denormalize(y, y_min: float, y_max: float) Arguments : y : tensor filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : tensor with denormalized values","title":"denormalize"},{"location":"estimator/estimator-class/#denormalize_np","text":"| @staticmethod | denormalize_np(y: np.ndarray, y_min: float, y_max: float) -> np.ndarray Arguments : y : numpy-array filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : numpy-array with denormalized values","title":"denormalize_np"},{"location":"estimator/estimator-class/#load_data_file","text":"| load_data_file(file_path: str, override: bool = False) -> Dict[str, np.ndarray] Method for loading the data from file. Arguments : file_path : Path for the file where the data is stored. Has to be a .csv or .npy file. override : Boolean whether to override already existing data. Returns : The data which is set for the Estimator.","title":"load_data_file"},{"location":"estimator/estimator-class/#set_data","text":"| set_data(loaded_data: np.ndarray, override: bool = False) Method for setting data and dependent values like max_card and input_length. Arguments : loaded_data : The data loaded from the file. override : Boolean whether to override already existing data.","title":"set_data"},{"location":"estimator/estimator-class/#split_data","text":"| split_data(split: float = 0.9) Function to split the data into training- and test-set by a parameterized split value. Arguments : split : Percentage of the data going into training set. (split=0.9 means 90% of data is training set)","title":"split_data"},{"location":"estimator/estimator-class/#train","text":"| train(epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1) -> Union[History, History] Method for training the before created Model. Arguments : epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per epoch. shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. validation_split : How much of the data should be taken as validation set -> these are taken from the training data, not the test data, and are reselected for every epoch. Returns : Training history as dict.","title":"train"},{"location":"estimator/estimator-class/#test","text":"| test() -> np.ndarray Let the trained neural network predict the test data. Returns : numpy-array containing the normalized predictions of the neural network for the test data","title":"test"},{"location":"estimator/estimator-class/#predict","text":"| predict(data: np.ndarray) -> np.ndarray Let the trained neural network predict the given data. Arguments : data : numpy-array containing at least one vectorized query which should be predicted Returns : numpy-array containing the normalized predictions of the neural network for the given data","title":"predict"},{"location":"estimator/estimator-class/#run","text":"| run(data_file_path: str = None, epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1, override_model: bool = False, save_model: bool = True, save_model_file_path: str = \"model\") -> np.ndarray Method for a full run of the Estimator, with training and testing. Arguments : data_file_path : Optional path to saved data file. Only necessary if no data has been set before. epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per epoch. shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. validation_split : How much of the data should be taken as validation set -> these are taken from the training data, not the test data, and are reselected for every epoch. override_model : Whether to override a probably already existing model. save_model : Whether to save the trained model to file. save_model_file_path : When save_model==True this parameter is required to give the path where the model should be saved. Returns : A numpy.ndarray containing the calculated q-error.","title":"run"},{"location":"estimator/estimator-class/#save_model","text":"| save_model(filename: str = \"model\") Method for saving the Model to file. Arguments : filename : Name of the file where the model should be stored. (Without file ending. \".h5\" is added to the filename)","title":"save_model"},{"location":"estimator/estimator-readme/","text":"Hello World!","title":"Estimator Readme"},{"location":"meta-collector/meta-collector-class/","text":"meta_collector.meta_collector CreationMode Objects class CreationMode(Enum) Enum for the different possibilities to use the MetaCollector. 0 -> don't create table, 1 -> create temporary table, 2 -> create permanent table MetaCollector Objects class MetaCollector() Class for MetaCollector. __init__ | __init__(database_connector: DatabaseConnector, debug: bool = True) Initializer for the MetaCollector Arguments : database_connector : The connector to the used database. debug : boolean whether to print additional information while processing get_columns_data_postgres | get_columns_data_postgres(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for PostgreSQL and MariaDB. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the data-type of the column, the tuple containing min value, max value and step size, the encodings if datatype is not int and the alternative column-name if the original one occurs more than once. get_columns_data_sqlite | get_columns_data_sqlite(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for SQLite. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the data-type of the column, the tuple containing min value, max value and step size, the encodings if datatype is not int and the alternative column-name if the original one occurs more than once. collect_min_max_step | collect_min_max_step(tablename: str, column: Tuple[str, str]) -> (Tuple[int, int, int], Dict) After collecting the datatype information for the columns this function returns the min and max values for the meta-table and the encoders. Arguments : tablename : String containing the name of the table where to find the column column : a tuple containing the name and the datatype for the column, each as string Returns : first: dictionary with the attribute-name as key and a tuple containing min-value, max-value and step-size (all as int) as value second: a dictionary of the not integer encoders with key attribute-name and value the encoder get_max_card | get_max_card(table_names: List[str or Tuple[str, str]], join_atts: List[str or Tuple[str, str]] = None) -> int Get the size of the join-table without any selections, the so called max-card. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case Returns : setup_view | setup_view(table_names: List[str or Tuple[str, str]], columns_types: List[Tuple], join_atts: List[str or Tuple[str, str]] = None, cube: bool = False, mode: CreationMode = CreationMode.NONE) -> (List[Tuple[str, str]], int) Create the tables tmpview and if cube==True also tmpview_cube containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns_types : columns to project on join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case cube : boolean whether to create the *_cube table, too mode : see CreationMode-Enum Returns : first: a list of tuples containing the name and the datatype for the columns, each as string second: the maximal cardinality as integer setup_cube_view | setup_cube_view(new_table_name: str, columns) Create the table tmpview_cube if cube==True containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : new_table_name : The name of the before created table. columns : Columns to project on. get_meta | get_meta(table_names: List[str or Tuple[str, str]], columns: List[str], join_atts: List[str or Tuple[str, str]] = None, mode: CreationMode = CreationMode.NONE, save: bool = True, save_file_name: str = None, batchmode: bool = False, cube: bool = False) -> Dict Method for the whole process of collecting the meta-information for the given tables joined on the given attributes and projected on the given columns. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : List of names of columns, as strings, to project on. join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case save : boolean whether to save the meta-information to file save_file_name : name for the save-file for the meta_information -> not needed if save==False batchmode : whether the meta data is collected in batches or not -> connection to db held open if batch mode mode : see CreationMode-Enum cube : Whether to create the _cube table additionally (only for CreationMode Temporary or Permanent). Returns : dictionary containing the meta-information get_meta_from_file | get_meta_from_file(file_path: str, save: bool = True, save_file_path: str = None, mode: CreationMode = CreationMode.NONE, override: bool = True) -> Dict[int, any] Method for collecting meta data for the information given in a file from QueryParser or at least a file formatted like this. Arguments : file_path : Path to the file. Format has to be the same like the output of QueryParser save : Whether to save the information to file or not. -> It is recommended to do so. save_file_path : Optional path for the save-file. mode : see CreationMode-Enum override : Whether to override an already existing meta_information file. Returns : The solution dict. save_meta | save_meta(meta_dict: Dict, file_name: str = \"meta_information\", mode: str = \"w\") Method for saving the meta-information to file. Arguments : meta_dict : the dictionary containing the meta-information to save file_name : the name (without file-type) for the save-file mode : The mode to open the file. Some common possibilities are 'w', 'w+', 'r', 'a', 'a+'","title":"Meta-Collector Class"},{"location":"meta-collector/meta-collector-class/#meta_collectormeta_collector","text":"","title":"meta_collector.meta_collector"},{"location":"meta-collector/meta-collector-class/#creationmode-objects","text":"class CreationMode(Enum) Enum for the different possibilities to use the MetaCollector. 0 -> don't create table, 1 -> create temporary table, 2 -> create permanent table","title":"CreationMode Objects"},{"location":"meta-collector/meta-collector-class/#metacollector-objects","text":"class MetaCollector() Class for MetaCollector.","title":"MetaCollector Objects"},{"location":"meta-collector/meta-collector-class/#__init__","text":"| __init__(database_connector: DatabaseConnector, debug: bool = True) Initializer for the MetaCollector Arguments : database_connector : The connector to the used database. debug : boolean whether to print additional information while processing","title":"__init__"},{"location":"meta-collector/meta-collector-class/#get_columns_data_postgres","text":"| get_columns_data_postgres(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for PostgreSQL and MariaDB. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the data-type of the column, the tuple containing min value, max value and step size, the encodings if datatype is not int and the alternative column-name if the original one occurs more than once.","title":"get_columns_data_postgres"},{"location":"meta-collector/meta-collector-class/#get_columns_data_sqlite","text":"| get_columns_data_sqlite(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for SQLite. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the data-type of the column, the tuple containing min value, max value and step size, the encodings if datatype is not int and the alternative column-name if the original one occurs more than once.","title":"get_columns_data_sqlite"},{"location":"meta-collector/meta-collector-class/#collect_min_max_step","text":"| collect_min_max_step(tablename: str, column: Tuple[str, str]) -> (Tuple[int, int, int], Dict) After collecting the datatype information for the columns this function returns the min and max values for the meta-table and the encoders. Arguments : tablename : String containing the name of the table where to find the column column : a tuple containing the name and the datatype for the column, each as string Returns : first: dictionary with the attribute-name as key and a tuple containing min-value, max-value and step-size (all as int) as value second: a dictionary of the not integer encoders with key attribute-name and value the encoder","title":"collect_min_max_step"},{"location":"meta-collector/meta-collector-class/#get_max_card","text":"| get_max_card(table_names: List[str or Tuple[str, str]], join_atts: List[str or Tuple[str, str]] = None) -> int Get the size of the join-table without any selections, the so called max-card. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case Returns :","title":"get_max_card"},{"location":"meta-collector/meta-collector-class/#setup_view","text":"| setup_view(table_names: List[str or Tuple[str, str]], columns_types: List[Tuple], join_atts: List[str or Tuple[str, str]] = None, cube: bool = False, mode: CreationMode = CreationMode.NONE) -> (List[Tuple[str, str]], int) Create the tables tmpview and if cube==True also tmpview_cube containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns_types : columns to project on join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case cube : boolean whether to create the *_cube table, too mode : see CreationMode-Enum Returns : first: a list of tuples containing the name and the datatype for the columns, each as string second: the maximal cardinality as integer","title":"setup_view"},{"location":"meta-collector/meta-collector-class/#setup_cube_view","text":"| setup_cube_view(new_table_name: str, columns) Create the table tmpview_cube if cube==True containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : new_table_name : The name of the before created table. columns : Columns to project on.","title":"setup_cube_view"},{"location":"meta-collector/meta-collector-class/#get_meta","text":"| get_meta(table_names: List[str or Tuple[str, str]], columns: List[str], join_atts: List[str or Tuple[str, str]] = None, mode: CreationMode = CreationMode.NONE, save: bool = True, save_file_name: str = None, batchmode: bool = False, cube: bool = False) -> Dict Method for the whole process of collecting the meta-information for the given tables joined on the given attributes and projected on the given columns. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : List of names of columns, as strings, to project on. join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case save : boolean whether to save the meta-information to file save_file_name : name for the save-file for the meta_information -> not needed if save==False batchmode : whether the meta data is collected in batches or not -> connection to db held open if batch mode mode : see CreationMode-Enum cube : Whether to create the _cube table additionally (only for CreationMode Temporary or Permanent). Returns : dictionary containing the meta-information","title":"get_meta"},{"location":"meta-collector/meta-collector-class/#get_meta_from_file","text":"| get_meta_from_file(file_path: str, save: bool = True, save_file_path: str = None, mode: CreationMode = CreationMode.NONE, override: bool = True) -> Dict[int, any] Method for collecting meta data for the information given in a file from QueryParser or at least a file formatted like this. Arguments : file_path : Path to the file. Format has to be the same like the output of QueryParser save : Whether to save the information to file or not. -> It is recommended to do so. save_file_path : Optional path for the save-file. mode : see CreationMode-Enum override : Whether to override an already existing meta_information file. Returns : The solution dict.","title":"get_meta_from_file"},{"location":"meta-collector/meta-collector-class/#save_meta","text":"| save_meta(meta_dict: Dict, file_name: str = \"meta_information\", mode: str = \"w\") Method for saving the meta-information to file. Arguments : meta_dict : the dictionary containing the meta-information to save file_name : the name (without file-type) for the save-file mode : The mode to open the file. Some common possibilities are 'w', 'w+', 'r', 'a', 'a+'","title":"save_meta"},{"location":"meta-collector/meta-collector-readme/","text":"Hello World!","title":"Meta-Collector Readme"},{"location":"querycommunicator/querycommunicator-class/","text":"query_communicator.query_communicator QueryCommunicator Objects class QueryCommunicator() Class for oberserving the generation and evaluation of queries, in order to have nullqueryfree set of queries if needed. Manages the communication between Evaluator and SQL Generator to get the required amount of queries if possible. The SQL_Generator itself is not able to find nullqueries, that are caused by a valid combination of attributes, which just don't match any data of the database. Vice Versa, the Evaluator is not able to generate new queries, if there are nullqueries. get_queries | get_queries(database_connector: DatabaseConnector, save_file_path: str, query_number: int) Function for generating queries and their cardinalities if nullqueries are allowed. Saves generated queries in ../assets/queries_with_cardinalities.csv Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns : get_nullfree_queries | get_nullfree_queries(query_number: int, save_file_path: str, database_connector: DatabaseConnector) Function that generates given number queries and their cardinalities which are not zero. There will be less queries then requested, if unavoidable. Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns : list of remained Queries reduce_queries | @staticmethod | reduce_queries(query_number: int, save_file_path: str) -> List Reduces genrated queries to the requested number of queries Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities Returns : DataFrame with reduced query sets write_queries | @staticmethod | write_queries(queries: List, save_file_path: str = 'assets/reduced_queries_with_cardinalities.csv') function for writing the csv file with the reduced queries Arguments : queries : list of queries to write in a csv file save_file_path : file path, where to save the file Returns : produce_queries | produce_queries(database_connector: DatabaseConnector, query_number: int = 10, nullqueries: bool = False, save_file_path: str = 'assets/reduced_queries_with_cardinalities.csv') Main function to produce the queries and return the correct csv file, depending if nullqueries are wanted or not Arguments : save_file_path : Path to save the finished query file nullqueries : decide whether to generate nullqueries or not, default: no nullqueries query_number : count of queries that are generated per meta file entry database_connector : Connector for the database connection, depending on the database system you are using Returns :","title":"Index"},{"location":"querycommunicator/querycommunicator-class/#query_communicatorquery_communicator","text":"","title":"query_communicator.query_communicator"},{"location":"querycommunicator/querycommunicator-class/#querycommunicator-objects","text":"class QueryCommunicator() Class for oberserving the generation and evaluation of queries, in order to have nullqueryfree set of queries if needed. Manages the communication between Evaluator and SQL Generator to get the required amount of queries if possible. The SQL_Generator itself is not able to find nullqueries, that are caused by a valid combination of attributes, which just don't match any data of the database. Vice Versa, the Evaluator is not able to generate new queries, if there are nullqueries.","title":"QueryCommunicator Objects"},{"location":"querycommunicator/querycommunicator-class/#get_queries","text":"| get_queries(database_connector: DatabaseConnector, save_file_path: str, query_number: int) Function for generating queries and their cardinalities if nullqueries are allowed. Saves generated queries in ../assets/queries_with_cardinalities.csv Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns :","title":"get_queries"},{"location":"querycommunicator/querycommunicator-class/#get_nullfree_queries","text":"| get_nullfree_queries(query_number: int, save_file_path: str, database_connector: DatabaseConnector) Function that generates given number queries and their cardinalities which are not zero. There will be less queries then requested, if unavoidable. Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns : list of remained Queries","title":"get_nullfree_queries"},{"location":"querycommunicator/querycommunicator-class/#reduce_queries","text":"| @staticmethod | reduce_queries(query_number: int, save_file_path: str) -> List Reduces genrated queries to the requested number of queries Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities Returns : DataFrame with reduced query sets","title":"reduce_queries"},{"location":"querycommunicator/querycommunicator-class/#write_queries","text":"| @staticmethod | write_queries(queries: List, save_file_path: str = 'assets/reduced_queries_with_cardinalities.csv') function for writing the csv file with the reduced queries Arguments : queries : list of queries to write in a csv file save_file_path : file path, where to save the file Returns :","title":"write_queries"},{"location":"querycommunicator/querycommunicator-class/#produce_queries","text":"| produce_queries(database_connector: DatabaseConnector, query_number: int = 10, nullqueries: bool = False, save_file_path: str = 'assets/reduced_queries_with_cardinalities.csv') Main function to produce the queries and return the correct csv file, depending if nullqueries are wanted or not Arguments : save_file_path : Path to save the finished query file nullqueries : decide whether to generate nullqueries or not, default: no nullqueries query_number : count of queries that are generated per meta file entry database_connector : Connector for the database connection, depending on the database system you are using Returns :","title":"produce_queries"},{"location":"querycommunicator/querycommunicator-class/databaseevaluator-class/","text":"","title":"DatabaseEvaluator Class"},{"location":"querycommunicator/querycommunicator-class/databaseevaluator-readme/","text":"Hello World!","title":"DatabaseEvaluator Readme"},{"location":"querycommunicator/querycommunicator-class/sqlgenerator-class/","text":"","title":"SQLGenerator Class"},{"location":"querycommunicator/querycommunicator-class/sqlgenerator-readme/","text":"SQL Generator This is a sub(sub)module for genrating sql queries, given a meta_information file. It's typically called from the Query communicator , which manages the hole process of generating queries and evaluating them. However, you can use the geneartor seperately to genrate queries without duplicates. Usage To initialize the generator, you need a meta_information file in .yaml file format. The file contains the columns you want to join, the attributes you want to select, the stepsize and min/max values of the columns, and so on. You can have more than one entry to generate queries. Note, that in the end there will be an own model for every entry in the meta_file and query_set respectively. If you want to generate more than one query set, note that there has to be an own entry with own ID in the meta file the columns will need a synonym as second argument Example file 0: columns: - - company_type_id - mc - integer - - 1 - 2 - 1 - {} - - info_type_id - mi_idx - integer - - 1 - 113 - 1 - {} - - production_year - t - integer - - 1874 - 2115 - 1 - {} join_attributes: - t.id=mc.movie_id - t.id=mi_idx.movie_id max_card: - 134163798 table_names: - - movie_companies - mc - - movie_info_idx - mi_idx - - title - t Given the meta File: Instantiate a new sql-generator with meta File python generator = SQLGenarator(config='meta_information_test.yaml') Generate the desired number of queries. Note that the default is 10(queries per entry in meta file) python generator.generate_queries(qnumber=100, save_readable = 'queries') You also have the possibility to specify the filename for the queries. Note that you just specify the name (e.g.'movie_queries') and not hte format. The Generator will safe a csv file (usable for vectorizer) and a human readable sql file with the specified name in the assets directory. Default is queries.csv/queries.sql .","title":"SQLGenerator Readme"},{"location":"querycommunicator/querycommunicator-class/sqlgenerator-readme/#sql-generator","text":"This is a sub(sub)module for genrating sql queries, given a meta_information file. It's typically called from the Query communicator , which manages the hole process of generating queries and evaluating them. However, you can use the geneartor seperately to genrate queries without duplicates.","title":"SQL Generator"},{"location":"querycommunicator/querycommunicator-class/sqlgenerator-readme/#usage","text":"To initialize the generator, you need a meta_information file in .yaml file format. The file contains the columns you want to join, the attributes you want to select, the stepsize and min/max values of the columns, and so on. You can have more than one entry to generate queries. Note, that in the end there will be an own model for every entry in the meta_file and query_set respectively. If you want to generate more than one query set, note that there has to be an own entry with own ID in the meta file the columns will need a synonym as second argument Example file 0: columns: - - company_type_id - mc - integer - - 1 - 2 - 1 - {} - - info_type_id - mi_idx - integer - - 1 - 113 - 1 - {} - - production_year - t - integer - - 1874 - 2115 - 1 - {} join_attributes: - t.id=mc.movie_id - t.id=mi_idx.movie_id max_card: - 134163798 table_names: - - movie_companies - mc - - movie_info_idx - mi_idx - - title - t Given the meta File: Instantiate a new sql-generator with meta File python generator = SQLGenarator(config='meta_information_test.yaml') Generate the desired number of queries. Note that the default is 10(queries per entry in meta file) python generator.generate_queries(qnumber=100, save_readable = 'queries') You also have the possibility to specify the filename for the queries. Note that you just specify the name (e.g.'movie_queries') and not hte format. The Generator will safe a csv file (usable for vectorizer) and a human readable sql file with the specified name in the assets directory. Default is queries.csv/queries.sql .","title":"Usage"},{"location":"queryparser/queryparser-class/","text":"query_parser.query_parser QueryParser Objects class QueryParser() Class for the query_parser. This is responsible of reading a given file and return a file containing the aggregated information of this file. read_file | read_file(file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Generic Method for reading the sql statements from a given .sql or a .csv file. Arguments : file_path : Path to the file containing the sql statements. This path has to end with .csv or .sql. No other file types are supported at the moment. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. query_format : The format of the sql query. Look at documentation of QueryFormat for details. :return read_sql_file | @staticmethod | read_sql_file(file_path: str, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Read the sql statements from given sql file. Arguments : file_path : Path to the file containing the sql statements. query_format : The format of the sql query. Look at documentation of QueryFormat for details. :return read_csv_file | @staticmethod | read_csv_file(file_path: str, inner_separator: str = \",\", outer_separator: str = \"#\") -> Tuple[Dict, str, str, str] Read the csv formatted sql statements from given file. Arguments : file_path : Path to the file containing the sql statements formatted as csv. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. :return create_solution_dict | create_solution_dict(command_dict: Dict[str, List[str] or List[Tuple[str, str]]], file_type: str, inner_separator: str) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for building the solution dict. Arguments : command_dict : Dict with a alphabetical sorted string of the joining tables as key and a list of where clauses as string if the file type is sql or a list of tuples containing the join-attribute-string in first and the selection-attribute-string in second place. file_type : String with 'csv'/'tsv' or 'sql' which tells the file type of the read file. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. :return The solution dict containing 'table_names', 'join_attributes' and 'selection_attributes'. table_name_unpacker | @staticmethod | table_name_unpacker(from_string: str, separator: str = \",\") -> List[Tuple[str, str]] Takes the sorted string of the from clause and extracts the tables with their aliases. Arguments : from_string : Alphabetical ordered string containing all tables to join, separated by the separator. separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : List of tuples where the first element of the tuple is the table name and the second one is the alias. sql_attribute_unpacker | sql_attribute_unpacker(where_string_list: List[str]) -> Tuple[List[str], List[str]] Unpack the attribute strings from sql-file into sets containing the attributes. Arguments : where_string_list : A list of strings from the where clauses. These have to be separated into join- and selection-attributes. Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second place. csv_attribute_unpacker | csv_attribute_unpacker(attribute_tuples: List[Tuple[str, str]], separator: str = \",\") -> Tuple[List[str], List[str]] Unpack the attribute strings from csv-file into sets containing the attributes. Arguments : attribute_tuples : A list of tuples of strings where the first string is the string for all join-attributes, while the second string contains all selection-attributes. separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second place. save_solution_dict | @staticmethod | save_solution_dict(solution_dict: Dict[int, Dict[str, List[str or Tuple[str, str]]]], save_file_path: str = \"solution_dict\") Save the solution to file with specified filename. Arguments : solution_dict : The dict containing the data to save. save_file_path : The path for the file in which the data should be saved. The .yaml ending is added automatically. run | run(file_path: str, save_file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for the whole parsing process. Arguments : file_path : The file to read in which the sql-statements are saved. save_file_path : The path where to save the results. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. query_format : The indicator for the format of the .sql query-file. If the given file is not .sql than this is not used. Returns :","title":"QueryParser Class"},{"location":"queryparser/queryparser-class/#query_parserquery_parser","text":"","title":"query_parser.query_parser"},{"location":"queryparser/queryparser-class/#queryparser-objects","text":"class QueryParser() Class for the query_parser. This is responsible of reading a given file and return a file containing the aggregated information of this file.","title":"QueryParser Objects"},{"location":"queryparser/queryparser-class/#read_file","text":"| read_file(file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Generic Method for reading the sql statements from a given .sql or a .csv file. Arguments : file_path : Path to the file containing the sql statements. This path has to end with .csv or .sql. No other file types are supported at the moment. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. query_format : The format of the sql query. Look at documentation of QueryFormat for details. :return","title":"read_file"},{"location":"queryparser/queryparser-class/#read_sql_file","text":"| @staticmethod | read_sql_file(file_path: str, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Read the sql statements from given sql file. Arguments : file_path : Path to the file containing the sql statements. query_format : The format of the sql query. Look at documentation of QueryFormat for details. :return","title":"read_sql_file"},{"location":"queryparser/queryparser-class/#read_csv_file","text":"| @staticmethod | read_csv_file(file_path: str, inner_separator: str = \",\", outer_separator: str = \"#\") -> Tuple[Dict, str, str, str] Read the csv formatted sql statements from given file. Arguments : file_path : Path to the file containing the sql statements formatted as csv. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. :return","title":"read_csv_file"},{"location":"queryparser/queryparser-class/#create_solution_dict","text":"| create_solution_dict(command_dict: Dict[str, List[str] or List[Tuple[str, str]]], file_type: str, inner_separator: str) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for building the solution dict. Arguments : command_dict : Dict with a alphabetical sorted string of the joining tables as key and a list of where clauses as string if the file type is sql or a list of tuples containing the join-attribute-string in first and the selection-attribute-string in second place. file_type : String with 'csv'/'tsv' or 'sql' which tells the file type of the read file. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. :return The solution dict containing 'table_names', 'join_attributes' and 'selection_attributes'.","title":"create_solution_dict"},{"location":"queryparser/queryparser-class/#table_name_unpacker","text":"| @staticmethod | table_name_unpacker(from_string: str, separator: str = \",\") -> List[Tuple[str, str]] Takes the sorted string of the from clause and extracts the tables with their aliases. Arguments : from_string : Alphabetical ordered string containing all tables to join, separated by the separator. separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : List of tuples where the first element of the tuple is the table name and the second one is the alias.","title":"table_name_unpacker"},{"location":"queryparser/queryparser-class/#sql_attribute_unpacker","text":"| sql_attribute_unpacker(where_string_list: List[str]) -> Tuple[List[str], List[str]] Unpack the attribute strings from sql-file into sets containing the attributes. Arguments : where_string_list : A list of strings from the where clauses. These have to be separated into join- and selection-attributes. Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second place.","title":"sql_attribute_unpacker"},{"location":"queryparser/queryparser-class/#csv_attribute_unpacker","text":"| csv_attribute_unpacker(attribute_tuples: List[Tuple[str, str]], separator: str = \",\") -> Tuple[List[str], List[str]] Unpack the attribute strings from csv-file into sets containing the attributes. Arguments : attribute_tuples : A list of tuples of strings where the first string is the string for all join-attributes, while the second string contains all selection-attributes. separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second place.","title":"csv_attribute_unpacker"},{"location":"queryparser/queryparser-class/#save_solution_dict","text":"| @staticmethod | save_solution_dict(solution_dict: Dict[int, Dict[str, List[str or Tuple[str, str]]]], save_file_path: str = \"solution_dict\") Save the solution to file with specified filename. Arguments : solution_dict : The dict containing the data to save. save_file_path : The path for the file in which the data should be saved. The .yaml ending is added automatically.","title":"save_solution_dict"},{"location":"queryparser/queryparser-class/#run","text":"| run(file_path: str, save_file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for the whole parsing process. Arguments : file_path : The file to read in which the sql-statements are saved. save_file_path : The path where to save the results. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. query_format : The indicator for the format of the .sql query-file. If the given file is not .sql than this is not used. Returns :","title":"run"},{"location":"queryparser/queryparser-readme/","text":"","title":"QueryParser Readme"},{"location":"vectorizer/vectorizer-class/","text":"vectorizer.vectorizer Vectorizer Objects class Vectorizer() Constructs a vector consisting of operator code and normalized value for each predicate in the sql query set with set_query method. __init__ | __init__() Intitialises the Vectorizer object by defining available operators. add_queries_with_cardinalities | add_queries_with_cardinalities(queries_with_cardinalities_path: str) Reads CSV file with format (querySetID;query;encodings;max_card;min_max_step;estimated_cardinality;true_cardinality) whereas min_max_step is an array of the format [[1, 2, 1], [1, 113, 1], [1878, 2115, 1]] sorted by lexicographic order of corresponding predicates and encodings is an empty array if only integer values are processed. For a querySetID all predicates are collected and sorted in lexicographical order to provide correct indices (e.g. in encodings & min_max_value) for a given predicate. Read queries are added to the list of vectorisation tasks. Arguments : queries_with_cardinalities_path : path to a CSV file containing all queries and their estimated and true cardinalities vectorize | vectorize() -> List[np.array] Vectorizes all vectorization tasks added. Returns : List of np.array vectors whereas each row contains the vectorized query and appended maximal, estimated and true cardinality (in this order) save | save(base_path: str, result_folder: str, base_filename: str, filetypes: str) Stores the SQL query and corresponding vector at given path as NPY and TXT file. Arguments : base_path : path to a directory for saving result_folder : name of folder to create for storing multiple files. This argument is seperated from base_path to empathize the need for an extra folder, since multiple files are saved. filename : filename without filetype. querySetID is appended for differentiation filetypes : string of file types must contain \"csv\" or \"npy\" vectorize_query_original vectorize_query_original(query: str, min_max: Dict[str, Tuple[int, int, int]], encoders: List[Dict[str, int]]) -> np.array Copy-pasted method of the original implementation for testing purposes; Only added Join detection Arguments : query : the query to vectorize min_max : dictionary of all min, max, step values for each predicate encoders : dictionary, which maps predicates to encoders Returns : the normalized vector without cardinalities vectorizer_tests vectorizer_tests() Test method to compare the original implementation with jupyter notebook output (truth) or with the Vectorizer implementation. Succeeds if no assertion throws an error.","title":"Vectorizer Class"},{"location":"vectorizer/vectorizer-class/#vectorizervectorizer","text":"","title":"vectorizer.vectorizer"},{"location":"vectorizer/vectorizer-class/#vectorizer-objects","text":"class Vectorizer() Constructs a vector consisting of operator code and normalized value for each predicate in the sql query set with set_query method.","title":"Vectorizer Objects"},{"location":"vectorizer/vectorizer-class/#__init__","text":"| __init__() Intitialises the Vectorizer object by defining available operators.","title":"__init__"},{"location":"vectorizer/vectorizer-class/#add_queries_with_cardinalities","text":"| add_queries_with_cardinalities(queries_with_cardinalities_path: str) Reads CSV file with format (querySetID;query;encodings;max_card;min_max_step;estimated_cardinality;true_cardinality) whereas min_max_step is an array of the format [[1, 2, 1], [1, 113, 1], [1878, 2115, 1]] sorted by lexicographic order of corresponding predicates and encodings is an empty array if only integer values are processed. For a querySetID all predicates are collected and sorted in lexicographical order to provide correct indices (e.g. in encodings & min_max_value) for a given predicate. Read queries are added to the list of vectorisation tasks. Arguments : queries_with_cardinalities_path : path to a CSV file containing all queries and their estimated and true cardinalities","title":"add_queries_with_cardinalities"},{"location":"vectorizer/vectorizer-class/#vectorize","text":"| vectorize() -> List[np.array] Vectorizes all vectorization tasks added. Returns : List of np.array vectors whereas each row contains the vectorized query and appended maximal, estimated and true cardinality (in this order)","title":"vectorize"},{"location":"vectorizer/vectorizer-class/#save","text":"| save(base_path: str, result_folder: str, base_filename: str, filetypes: str) Stores the SQL query and corresponding vector at given path as NPY and TXT file. Arguments : base_path : path to a directory for saving result_folder : name of folder to create for storing multiple files. This argument is seperated from base_path to empathize the need for an extra folder, since multiple files are saved. filename : filename without filetype. querySetID is appended for differentiation filetypes : string of file types must contain \"csv\" or \"npy\"","title":"save"},{"location":"vectorizer/vectorizer-class/#vectorize_query_original","text":"vectorize_query_original(query: str, min_max: Dict[str, Tuple[int, int, int]], encoders: List[Dict[str, int]]) -> np.array Copy-pasted method of the original implementation for testing purposes; Only added Join detection Arguments : query : the query to vectorize min_max : dictionary of all min, max, step values for each predicate encoders : dictionary, which maps predicates to encoders Returns : the normalized vector without cardinalities","title":"vectorize_query_original"},{"location":"vectorizer/vectorizer-class/#vectorizer_tests","text":"vectorizer_tests() Test method to compare the original implementation with jupyter notebook output (truth) or with the Vectorizer implementation. Succeeds if no assertion throws an error.","title":"vectorizer_tests"},{"location":"vectorizer/vectorizer-readme/","text":"Vectorizer This submodule uses the output of the postgres-evaluater submodule to encode the SQL query into a vector and also normalizes the cardinalities given. Vector and cardinalities are input to the estimator submodule. Usage Normally this submodule is called from main.py , however you may want to use it separately: First, you need a CSV file (semicolon separated) with the queries, meta data and estimated and true cardinalities. E.g. queries_with_cardinalities.csv : querySetID;query;encodings;max_card;min_max_step;estimated_cardinality;true_cardinality 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<2 AND mi_idx.info_type_id=107 AND t.production_year>2009;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];197595;1152438 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=1 AND mi_idx.info_type_id<80 AND t.production_year<=1894;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];30903;1416 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=1 AND mi_idx.info_type_id!=62 AND t.production_year<=2094;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];70814563;94395272 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id>45 AND t.production_year<1939;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];652856;197336 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=32 AND t.production_year<=1918;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];1733520;1054332 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id<54 AND t.production_year<=2097;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];81368212;30387086 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=2 AND mi_idx.info_type_id>=38 AND t.production_year<1896;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];362;2501 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=66 AND t.production_year=2026;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];42743;14 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=1 AND mk.keyword_id>=117023 AND t.production_year<=1894;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];436;26 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>=1 AND mk.keyword_id<35239 AND t.production_year<=1896;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];18935;2117 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id<2 AND mk.keyword_id<=35888 AND t.production_year!=2020;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];14700094;40290318 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=2 AND mk.keyword_id>196933 AND t.production_year=1907;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];101;2 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id!=1 AND mk.keyword_id<19712 AND t.production_year<1980;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];2728149;1552444 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>1 AND mk.keyword_id<=186471 AND t.production_year<2110;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];24858350;14307838 encodings and min_max_step are arrays in string representation querySetID is mandatory, since it is used for automatically determine the length of the output vector. All vectors of the same querySet must have the same length. Given one or more such CSV files: Instantiate a new vectorizer python vectorizer = Vectorizer() Add as many CSV files with queries, meta data and cardinalities as you want python vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_1.csv\") vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_2.csv\") Vectorize all queries within the CSV files and normalize the cardinalities python vectors = vectorizer.vectorize() The resulting matrix contains for each row the vector, normalized estimated cardinality and normalized true cardinality python for vec in vectors: vectorized_query, cardinality_estimation, cardinality_true = vec[:-2], vec[-2], vec[-1] You may now want to save the matrix as .npy and .csv file python vectorizer.save(\"/path/to/directory/matrix\", \"csv\") E.g.: matrix.csv : 0,1,0,0,1,0,0,1,0.946902654867256666,0,1,0,0.554621848739495826,134163798,0.651576470484740322,0.745803338052605902 0,0,0,1,0.5,1,0,0,0.707964601769911495,1,0,1,0.0714285714285714246,134163798,0.552436280887511844,0.387697419969840307 0,1,0,1,0.5,1,1,0,0.548672566371681381,1,0,1,0.911764705882352922,134163798,0.9658556575333751,0.981214080678194711 0,0,1,1,1,0,1,0,0.39823008849557523,1,0,0,0.260504201680672287,134163798,0.715437781548679874,0.651506384900475854 0,0,0,1,1,1,0,1,0.283185840707964598,1,0,1,0.172268907563025209,134163798,0.767619189647782418,0.7410491653476593 0,0,1,1,1,1,0,0,0.477876106194690287,1,0,1,0.924369747899159711,134163798,0.973278750577822982,0.920647733098342469 0,1,0,1,1,0,1,1,0.336283185840707988,1,0,0,0.0798319327731092376,134163798,0.314815867372580604,0.418093768713123426 0,0,0,1,1,1,0,1,0.584070796460177011,0,0,1,0.626050420168067223,134163798,0.569767811257714807,0.141016173481957524 1,0,0,1,0.5,0,1,1,0.494546269022554497,1,0,1,0.0714285714285714246,63056995,0.338407275959206999,0.181413043100808496 1,0,1,1,0.5,1,0,0,0.148922143288804737,1,0,1,0.0798319327731092376,63056995,0.548386100028571355,0.426389049816630394 1,1,0,0,1,1,0,1,0.151664856504118289,1,1,0,0.600840336134453756,63056995,0.918918617255151005,0.975059073360462714 1,0,0,1,1,0,1,0,0.832250757521331042,0,0,1,0.126050420168067223,63056995,0.256973066165060549,0.0385949089828031763 1,1,1,0,0.5,1,0,0,0.0833041030820658723,1,0,0,0.432773109243697496,63056995,0.825139509620369638,0.793747135768320677 1,0,1,0,0.5,1,0,1,0.7880377133632257,1,0,0,0.978991596638655426,63056995,0.948169897872308987,0.917412655803315658 * Whole code: python vectorizer = Vectorizer() vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_1.csv\") vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_2.csv\") vectors = vectorizer.vectorize() for vec in vectors: vectorized_query, cardinality_estimation, cardinality_true = vec[:-2], vec[-2], vec[-1] vectorizer.save(\"/path/to/directory/filename\", \"csv\")","title":"Vectorizer Readme"},{"location":"vectorizer/vectorizer-readme/#vectorizer","text":"This submodule uses the output of the postgres-evaluater submodule to encode the SQL query into a vector and also normalizes the cardinalities given. Vector and cardinalities are input to the estimator submodule.","title":"Vectorizer"},{"location":"vectorizer/vectorizer-readme/#usage","text":"Normally this submodule is called from main.py , however you may want to use it separately: First, you need a CSV file (semicolon separated) with the queries, meta data and estimated and true cardinalities. E.g. queries_with_cardinalities.csv : querySetID;query;encodings;max_card;min_max_step;estimated_cardinality;true_cardinality 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<2 AND mi_idx.info_type_id=107 AND t.production_year>2009;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];197595;1152438 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=1 AND mi_idx.info_type_id<80 AND t.production_year<=1894;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];30903;1416 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=1 AND mi_idx.info_type_id!=62 AND t.production_year<=2094;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];70814563;94395272 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id>45 AND t.production_year<1939;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];652856;197336 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=32 AND t.production_year<=1918;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];1733520;1054332 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id>=2 AND mi_idx.info_type_id<54 AND t.production_year<=2097;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];81368212;30387086 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id<=2 AND mi_idx.info_type_id>=38 AND t.production_year<1896;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];362;2501 0;SELECT COUNT(*) FROM movie_companies mc,movie_info_idx mi_idx,title t WHERE t.id=mi_idx.movie_id AND t.id=mc.movie_id AND mc.company_type_id=2 AND mi_idx.info_type_id<=66 AND t.production_year=2026;[];134163798;[[1, 2, 1], [1, 113, 1], [1878, 2115, 1]];42743;14 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=1 AND mk.keyword_id>=117023 AND t.production_year<=1894;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];436;26 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>=1 AND mk.keyword_id<35239 AND t.production_year<=1896;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];18935;2117 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id<2 AND mk.keyword_id<=35888 AND t.production_year!=2020;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];14700094;40290318 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id=2 AND mk.keyword_id>196933 AND t.production_year=1907;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];101;2 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id!=1 AND mk.keyword_id<19712 AND t.production_year<1980;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];2728149;1552444 1;SELECT COUNT(*) FROM movie_companies mc,movie_keyword mk,title t WHERE t.id=mc.movie_id AND t.id=mk.movie_id AND mc.company_type_id>1 AND mk.keyword_id<=186471 AND t.production_year<2110;[];63056995;[[1, 2, 1], [1, 236627, 1], [1878, 2115, 1]];24858350;14307838 encodings and min_max_step are arrays in string representation querySetID is mandatory, since it is used for automatically determine the length of the output vector. All vectors of the same querySet must have the same length. Given one or more such CSV files: Instantiate a new vectorizer python vectorizer = Vectorizer() Add as many CSV files with queries, meta data and cardinalities as you want python vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_1.csv\") vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_2.csv\") Vectorize all queries within the CSV files and normalize the cardinalities python vectors = vectorizer.vectorize() The resulting matrix contains for each row the vector, normalized estimated cardinality and normalized true cardinality python for vec in vectors: vectorized_query, cardinality_estimation, cardinality_true = vec[:-2], vec[-2], vec[-1] You may now want to save the matrix as .npy and .csv file python vectorizer.save(\"/path/to/directory/matrix\", \"csv\") E.g.: matrix.csv : 0,1,0,0,1,0,0,1,0.946902654867256666,0,1,0,0.554621848739495826,134163798,0.651576470484740322,0.745803338052605902 0,0,0,1,0.5,1,0,0,0.707964601769911495,1,0,1,0.0714285714285714246,134163798,0.552436280887511844,0.387697419969840307 0,1,0,1,0.5,1,1,0,0.548672566371681381,1,0,1,0.911764705882352922,134163798,0.9658556575333751,0.981214080678194711 0,0,1,1,1,0,1,0,0.39823008849557523,1,0,0,0.260504201680672287,134163798,0.715437781548679874,0.651506384900475854 0,0,0,1,1,1,0,1,0.283185840707964598,1,0,1,0.172268907563025209,134163798,0.767619189647782418,0.7410491653476593 0,0,1,1,1,1,0,0,0.477876106194690287,1,0,1,0.924369747899159711,134163798,0.973278750577822982,0.920647733098342469 0,1,0,1,1,0,1,1,0.336283185840707988,1,0,0,0.0798319327731092376,134163798,0.314815867372580604,0.418093768713123426 0,0,0,1,1,1,0,1,0.584070796460177011,0,0,1,0.626050420168067223,134163798,0.569767811257714807,0.141016173481957524 1,0,0,1,0.5,0,1,1,0.494546269022554497,1,0,1,0.0714285714285714246,63056995,0.338407275959206999,0.181413043100808496 1,0,1,1,0.5,1,0,0,0.148922143288804737,1,0,1,0.0798319327731092376,63056995,0.548386100028571355,0.426389049816630394 1,1,0,0,1,1,0,1,0.151664856504118289,1,1,0,0.600840336134453756,63056995,0.918918617255151005,0.975059073360462714 1,0,0,1,1,0,1,0,0.832250757521331042,0,0,1,0.126050420168067223,63056995,0.256973066165060549,0.0385949089828031763 1,1,1,0,0.5,1,0,0,0.0833041030820658723,1,0,0,0.432773109243697496,63056995,0.825139509620369638,0.793747135768320677 1,0,1,0,0.5,1,0,1,0.7880377133632257,1,0,0,0.978991596638655426,63056995,0.948169897872308987,0.917412655803315658 * Whole code: python vectorizer = Vectorizer() vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_1.csv\") vectorizer.add_queries_with_cardinalities(\"queries_with_cardinalities_2.csv\") vectors = vectorizer.vectorize() for vec in vectors: vectorized_query, cardinality_estimation, cardinality_true = vec[:-2], vec[-2], vec[-1] vectorizer.save(\"/path/to/directory/filename\", \"csv\")","title":"Usage"}]}