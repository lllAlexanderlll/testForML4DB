{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Local Cardinality Estimation KP SS2020 The submodule 'meta-collector' collects the several informations from the requested table and saves the information into a .json file The submodule 'sql-generator' uses the output of the meta-collector to create random SQL-Queries with the corresponding schema The submodule 'vectorizer' uses the output of the sql-generator to encode it into a vectors The submodule 'estimator' takes the encoded vectors and uses them on a neural network The submodule 'postrgres-evaluator' takes the sql-queries and executes them on the postgres-database to get the true cardinality For building the Documentation you need to execute the setup_doc.sh . This script installs the prerequisites if not already installed, builds the documentation and starts the documentation-server.","title":"Home"},{"location":"#local-cardinality-estimation","text":"KP SS2020 The submodule 'meta-collector' collects the several informations from the requested table and saves the information into a .json file The submodule 'sql-generator' uses the output of the meta-collector to create random SQL-Queries with the corresponding schema The submodule 'vectorizer' uses the output of the sql-generator to encode it into a vectors The submodule 'estimator' takes the encoded vectors and uses them on a neural network The submodule 'postrgres-evaluator' takes the sql-queries and executes them on the postgres-database to get the true cardinality For building the Documentation you need to execute the setup_doc.sh . This script installs the prerequisites if not already installed, builds the documentation and starts the documentation-server.","title":"Local Cardinality Estimation"},{"location":"api-documentation/","text":"main estimator estimator.estimator Estimator Objects class Estimator() Class containing the neural network for cardinality estimation. The specifications of the neural network can be changed in 'config.yaml'. __init__ | __init__(config: Dict[str, Any] = None, config_file_path: str = \"config.yaml\", data: np.ndarray = None, model: Model = None, model_path: str = None, debug: bool = True) Initializer for the Estimator. Configuration options for the neural network are optionally passed via a config dict. It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". Arguments : config : Only used if neither a model or a model_path is passed. if given: It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". if not given: the config file 'config.yaml' is used for these settings. config_file_path : path for the config-file -> only necessary if no config is given data : Optional parameter for giving the data for training and testing. If given it has to be a Dict with at least \"x\" and \"y\" and optionally \"postgres_estimate\" as keys. The values have to be numpy.ndarray. For key \"x\" it should be the vectorized queries, for key \"y\" the true cardinalities in the same order and for optional key \"postgres_estimate\" the estimates of the postgres optimizer for the query. model : Option to pass a Model which can be used. model_path : Option to pass a path to a saved model in an .h5 file. debug : Boolean whether to print additional information while processing. get_model | get_model(len_input: int, override: bool = False) -> Model Function for creating the model of the neural network with the information from self.config Arguments : len_input : The size of the input vector. override : Whether an existing model should be overridden. Returns : The model for the neural network with the given properties. load_model | load_model(model_path: str) Method for loading an already existing model wich was saved to file. Arguments : model_path : Path to the file containing the model to load denormalize | @staticmethod | denormalize(y, y_min: float, y_max: float) Arguments : y : tensor filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : tensor with denormalized values denormalize_np | @staticmethod | denormalize_np(y: np.ndarray, y_min: float, y_max: float) -> np.ndarray Arguments : y : numpy-array filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : numpy-array with denormalized values load_data_file | load_data_file(file_path: str, override: bool = False) -> Dict[str, np.ndarray] Method for loading the data from file. Arguments : file_path : Path for the file where the data is stored. Has to be a .csv or .npy file. override : Boolean whether to override already existing data. Returns : The data which is set for the Estimator. set_data | set_data(loaded_data: np.ndarray, override: bool = False) Method for setting data and dependent values like max_card and input_length. Arguments : loaded_data : The data loaded from the file. override : Boolean whether to override already existing data. split_data | split_data(split: float = 0.9) Function to split the data into training- and test-set by a parameterized split value. Arguments : split : Percentage of the data going into training set. (split=0.9 means 90% of data is training set) train | train(epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1) -> Union[History, History] Method for training the before created Model. Arguments : epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per epoch. shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. validation_split : How much of the data should be taken as validation set -> these are taken from the training data, not the test data, and are reselected for every epoch. Returns : Training history as dict. test | test() -> np.ndarray Let the trained neural network predict the test data. Returns : numpy-array containing the normalized predictions of the neural network for the test data predict | predict(data: np.ndarray) -> np.ndarray Let the trained neural network predict the given data. Arguments : data : numpy-array containing at least one vectorized query which should be predicted Returns : numpy-array containing the normalized predictions of the neural network for the given data run | run(data_file_path: str = None, epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1, override_model: bool = False, save_model: bool = True, save_model_file_path: str = \"model\") -> np.ndarray Method for a full run of the Estimator, with training and testing. Arguments : data_file_path : Optional path to saved data file. Only necessary if no data has been set before. epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per epoch. shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. validation_split : How much of the data should be taken as validation set -> these are taken from the training data, not the test data, and are reselected for every epoch. override_model : Whether to override a probably already existing model. save_model : Whether to save the trained model to file. save_model_file_path : When save_model==True this parameter is required to give the path where the model should be saved. Returns : A numpy.ndarray containing the calculated q-error. save_model | save_model(filename: str = \"model\") Method for saving the Model to file. Arguments : filename : Name of the file where the model should be stored. (Without file ending. \".h5\" is added to the filename) query_parser query_parser.query_parser QueryParser Objects class QueryParser() Class for the query_parser. This is responsible of reading a given file and return a file containing the aggregated information of this file. read_file | read_file(file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Generic Method for reading the sql statements from a given .sql or a .csv file. Arguments : file_path : Path to the file containing the sql statements. This path has to end with .csv or .sql. No other file types are supported at the moment. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. query_format : The format of the sql query. Look at documentation of QueryFormat for details. :return read_sql_file | @staticmethod | read_sql_file(file_path: str, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Read the sql statements from given sql file. Arguments : file_path : Path to the file containing the sql statements. query_format : The format of the sql query. Look at documentation of QueryFormat for details. :return read_csv_file | @staticmethod | read_csv_file(file_path: str, inner_separator: str = \",\", outer_separator: str = \"#\") -> Tuple[Dict, str, str, str] Read the csv formatted sql statements from given file. Arguments : file_path : Path to the file containing the sql statements formatted as csv. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. :return create_solution_dict | create_solution_dict(command_dict: Dict[str, List[str] or List[Tuple[str, str]]], file_type: str, inner_separator: str) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for building the solution dict. Arguments : command_dict : Dict with a alphabetical sorted string of the joining tables as key and a list of where clauses as string if the file type is sql or a list of tuples containing the join-attribute-string in first and the selection-attribute-string in second place. file_type : String with 'csv'/'tsv' or 'sql' which tells the file type of the read file. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. :return The solution dict containing 'table_names', 'join_attributes' and 'selection_attributes'. table_name_unpacker | @staticmethod | table_name_unpacker(from_string: str, separator: str = \",\") -> List[Tuple[str, str]] Takes the sorted string of the from clause and extracts the tables with their aliases. Arguments : from_string : Alphabetical ordered string containing all tables to join, separated by the separator. separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : List of tuples where the first element of the tuple is the table name and the second one is the alias. sql_attribute_unpacker | sql_attribute_unpacker(where_string_list: List[str]) -> Tuple[List[str], List[str]] Unpack the attribute strings from sql-file into sets containing the attributes. Arguments : where_string_list : A list of strings from the where clauses. These have to be separated into join- and selection-attributes. Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second place. csv_attribute_unpacker | csv_attribute_unpacker(attribute_tuples: List[Tuple[str, str]], separator: str = \",\") -> Tuple[List[str], List[str]] Unpack the attribute strings from csv-file into sets containing the attributes. Arguments : attribute_tuples : A list of tuples of strings where the first string is the string for all join-attributes, while the second string contains all selection-attributes. separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second place. save_solution_dict | @staticmethod | save_solution_dict(solution_dict: Dict[int, Dict[str, List[str or Tuple[str, str]]]], save_file_path: str = \"solution_dict\") Save the solution to file with specified filename. Arguments : solution_dict : The dict containing the data to save. save_file_path : The path for the file in which the data should be saved. The .yaml ending is added automatically. run | run(file_path: str, save_file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for the whole parsing process. Arguments : file_path : The file to read in which the sql-statements are saved. save_file_path : The path where to save the results. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. query_format : The indicator for the format of the .sql query-file. If the given file is not .sql than this is not used. Returns : database_connector database_connector.database_connector Database Objects class Database(Enum) Enum for the different supported databases. If you use MySQL, then use MARIADB as value here. DatabaseConnector Objects class DatabaseConnector() Class for DatabaseConnector. __init__ | __init__(database: Database, debug: bool = True) Initializer for the DatabaseConnector Arguments : debug : boolean whether to print additional information while processing connect | connect(config: Dict = None, config_file_path: str = None, sqlite_file_path: str = None) Wrapper method for connecting to the selected database. Arguments : config : if given: It has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 5432 if not given) for PostgreSQL or it has to be a dictionary with at least database, user and password and optionally host and port (default to host: localhost, port: 3306 if not given) for MariaDB. if not given: The config file path is needed and used for these settings. config_file_path : Path to the config file for PostgreSQL or MariaDB. sqlite_file_path : Path to the SQLite database file. connect_to_postgres | connect_to_postgres(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : config : if given: it has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 5432 if not given) if not given: the config file 'config.yaml' is used for these settings config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a .yaml/.yml file connect_to_mariadb | connect_to_mariadb(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : config : if given: it has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 3306 if not given) if not given: the config file 'config.yaml' is used for these settings config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a .yaml/.yml file connect_to_sqlite | connect_to_sqlite(database_file_path: str) Open connection to a sqlite database. Arguments : database_file_path : The path to the sqlite database file. close_database_connection | close_database_connection() close the connection to the database Returns : void execute | execute(sql_string: str) Method for executing a SQL-Query. Arguments : sql_string : The SQL-Query to execute fetchall | fetchall() Wrapper for fetchall method. fetchone | fetchone() Wrapper for fetchone method. vectorizer vectorizer.vectorizer Vectorizer Objects class Vectorizer() Constructs a vector consisting of operator code and normalized value for each predicate in the sql query set with set_query method. __init__ | __init__() Intitialises the Vectorizer object by defining available operators. add_queries_with_cardinalities | add_queries_with_cardinalities(queries_with_cardinalities_path: str) Reads CSV file with format (querySetID;query;encodings;max_card;min_max_step;estimated_cardinality;true_cardinality) whereas min_max_step is an array of the format [[1, 2, 1], [1, 113, 1], [1878, 2115, 1]] sorted by lexicographic order of corresponding predicates and encodings is an empty array if only integer values are processed. For a querySetID all predicates are collected and sorted in lexicographical order to provide correct indices (e.g. in encodings & min_max_value) for a given predicate. Read queries are added to the list of vectorisation tasks. Arguments : queries_with_cardinalities_path : path to a CSV file containing all queries and their estimated and true cardinalities vectorize | vectorize() -> List[np.array] Vectorizes all vectorization tasks added. Returns : List of np.array vectors whereas each row contains the vectorized query and appended maximal, estimated and true cardinality (in this order) save | save(base_path: str, result_folder: str, base_filename: str, filetypes: str) Stores the SQL query and corresponding vector at given path as NPY and TXT file. Arguments : base_path : path to a directory for saving result_folder : name of folder to create for storing multiple files. This argument is seperated from base_path to empathize the need for an extra folder, since multiple files are saved. filename : filename without filetype. querySetID is appended for differentiation filetypes : string of file types must contain \"csv\" or \"npy\" vectorize_query_original vectorize_query_original(query: str, min_max: Dict[str, Tuple[int, int, int]], encoders: List[Dict[str, int]]) -> np.array Copy-pasted method of the original implementation for testing purposes; Only added Join detection Arguments : query : the query to vectorize min_max : dictionary of all min, max, step values for each predicate encoders : dictionary, which maps predicates to encoders Returns : the normalized vector without cardinalities vectorizer_tests vectorizer_tests() Test method to compare the original implementation with jupyter notebook output (truth) or with the Vectorizer implementation. Succeeds if no assertion throws an error. query_communicator query_communicator.sql_generator query_communicator.sql_generator.sql_generator SQLGenerator Objects class SQLGenerator() Class for generating SQL queries. Uses Meta Information from MetaCollector Step before. __init__ | __init__(config: str = None, debug: bool = False) Initializer of the SQL Generator. Reads the needed parameters from the meta-information.yaml generated from the MetaCollector which was possibly executed before. It's also possible to pass an own meta.yaml file to skip the metaCollector Step. Arguments : config : Config file which contains columns with their datatypes, tables, join_attributes, encodings, maximum cardinality min and max values of the columns with the step size. If None: meta-information.yaml generated from MetaCollector is used write_sql | @staticmethod | write_sql(queries: List[Tuple[int, str]], file: str) Function for writing a human readable sql file with the generated queries Arguments : queries : list of queries, containing querySetID and query as tuple file : file name for sql file Returns : random_operator_value | @staticmethod | random_operator_value(range: List[int], val_type: str, encoding: dict = None) -> Tuple[str, str or int] Function for random operator and value creation from a list of allowed operators and a range of values Arguments : range : Min_max Value range and stepsize of a given column from meta_information val_type : Type of column, can be either integer or enumeration. when enumeration, then an encoding dict has to be given encoding : Encoding of the possible values for the column Returns : Tuple consisting of operator and value as string or int generate_queries | generate_queries(qnumber: int = 10, save_readable: str = 'queries') Generates given number of SQL Queries with given meta-information. Function, which models the hole process of query generation Arguments : self : save_readable : Saves the generated SQL queries into a human friendly readable .sql file qnumber : Number of generated queries per meta-entry Returns : list with queries as String query_communicator.query_communicator QueryCommunicator Objects class QueryCommunicator() Class for oberserving the generation and evaluation of queries, in order to have nullqueryfree set of queries if needed. Manages the communication between Evaluator and SQL Generator to get the required amount of queries if possible. The SQL_Generator itself is not able to find nullqueries, that are caused by a valid combination of attributes, which just don't match any data of the database. Vice Versa, the Evaluator is not able to generate new queries, if there are nullqueries. get_queries | get_queries(database_connector: DatabaseConnector, save_file_path: str, query_number: int) Function for generating queries and their cardinalities if nullqueries are allowed. Saves generated queries in ../assets/queries_with_cardinalities.csv Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns : get_nullfree_queries | get_nullfree_queries(query_number: int, save_file_path: str, database_connector: DatabaseConnector) Function that generates given number queries and their cardinalities which are not zero. There will be less queries then requested, if unavoidable. Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns : list of remained Queries reduce_queries | @staticmethod | reduce_queries(query_number: int, save_file_path: str) -> List Reduces genrated queries to the requested number of queries Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities Returns : DataFrame with reduced query sets write_queries | @staticmethod | write_queries(queries: List, save_file_path: str = 'assets/reduced_queries_with_cardinalities.csv') function for writing the csv file with the reduced queries Arguments : queries : list of queries to write in a csv file save_file_path : file path, where to save the file Returns : produce_queries | produce_queries(database_connector: DatabaseConnector, query_number: int = 10, nullqueries: bool = False, save_file_path: str = 'assets/reduced_queries_with_cardinalities.csv') Main function to produce the queries and return the correct csv file, depending if nullqueries are wanted or not Arguments : save_file_path : Path to save the finished query file nullqueries : decide whether to generate nullqueries or not, default: no nullqueries query_number : count of queries that are generated per meta file entry database_connector : Connector for the database connection, depending on the database system you are using Returns : query_communicator.database_evaluator query_communicator.database_evaluator.database_evaluator DatabaseEvaluator Objects class DatabaseEvaluator() Class for DatabaseEvaluator. Using psycopg2 to establish a connection to the postgres database. Evaluate true and estimated cardinalities from q given query list and save them. __init__ | __init__(database_connector: DatabaseConnector, debug: bool = True, input_file_name: str = 'queries.csv') Initializer for the DatabaseEvaluator Configuration options for the database are optionally passed via a config dict. It must contain at least the dbname, the username and the password. Additionally the host and the port can be given if there not default (host: localhost, port: 5432). Arguments : debug : boolean whether to print additional information while processing input_file_name : name of the file used for the sql query import, have to be .csv or .sql and located in the asset folder database_connector : Handles the database connection to the desired database. import_sql_queries | import_sql_queries(path) load the queries from sql or csv file, wich is provided by the sql_generator submodule Arguments : path : path to the file with the given queries (per default in asset folder), relative to the database_evaluator folder Returns : void generate_explain_queries | generate_explain_queries() generate EXPLAIN sql statements for cardinality estimation Returns : void get_true_cardinalities | get_true_cardinalities() execute the given queries against the database and calculate the true cardinality from each query Returns : void get_estimated_cardinalities | get_estimated_cardinalities() execute the adapted queries against the database and calculate the postgres cardinality estimation for each query Returns : void save_cardinalities | save_cardinalities(save_readable: Tuple[bool, str] = (True, 'assets/queries_with_cardinalities.txt'), save_file_path: str = 'assets/queries_with_cardinalities.csv', eliminate_null_queries: bool = True) execute the adapted queries against the database and calculate the postgres cardinality estimation for each query Arguments : eliminate_null_queries : if True only queries with true cardinality > 0 will be saved save_readable : if True: save queries and corresponing cardinalities human readable in an separate text file, per default as assets/queries_with_cardinalities.txt save_file_path : path to save the finished queries with their cardinalities Returns : void get_cardinalities | get_cardinalities(eliminate_null_queries: bool = True, save_file_path: str = 'assets/queries_with_cardinalities.csv') function that manage the whole process of cardinality estimation/calculation Arguments : eliminate_null_queries : if True only queries with true cardinality > 0 will be saved save_file_path : path to save the finished queries with their cardinalities Returns : void meta_collector meta_collector.meta_collector CreationMode Objects class CreationMode(Enum) Enum for the different possibilities to use the MetaCollector. 0 -> don't create table, 1 -> create temporary table, 2 -> create permanent table MetaCollector Objects class MetaCollector() Class for MetaCollector. __init__ | __init__(database_connector: DatabaseConnector, debug: bool = True) Initializer for the MetaCollector Arguments : database_connector : The connector to the used database. debug : boolean whether to print additional information while processing get_columns_data_postgres | get_columns_data_postgres(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for PostgreSQL and MariaDB. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the data-type of the column, the tuple containing min value, max value and step size, the encodings if datatype is not int and the alternative column-name if the original one occurs more than once. get_columns_data_sqlite | get_columns_data_sqlite(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for SQLite. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the data-type of the column, the tuple containing min value, max value and step size, the encodings if datatype is not int and the alternative column-name if the original one occurs more than once. collect_min_max_step | collect_min_max_step(tablename: str, column: Tuple[str, str]) -> (Tuple[int, int, int], Dict) After collecting the datatype information for the columns this function returns the min and max values for the meta-table and the encoders. Arguments : tablename : String containing the name of the table where to find the column column : a tuple containing the name and the datatype for the column, each as string Returns : first: dictionary with the attribute-name as key and a tuple containing min-value, max-value and step-size (all as int) as value second: a dictionary of the not integer encoders with key attribute-name and value the encoder get_max_card | get_max_card(table_names: List[str or Tuple[str, str]], join_atts: List[str or Tuple[str, str]] = None) -> int Get the size of the join-table without any selections, the so called max-card. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case Returns : setup_view | setup_view(table_names: List[str or Tuple[str, str]], columns_types: List[Tuple], join_atts: List[str or Tuple[str, str]] = None, cube: bool = False, mode: CreationMode = CreationMode.NONE) -> (List[Tuple[str, str]], int) Create the tables tmpview and if cube==True also tmpview_cube containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns_types : columns to project on join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case cube : boolean whether to create the *_cube table, too mode : see CreationMode-Enum Returns : first: a list of tuples containing the name and the datatype for the columns, each as string second: the maximal cardinality as integer setup_cube_view | setup_cube_view(new_table_name: str, columns) Create the table tmpview_cube if cube==True containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : new_table_name : The name of the before created table. columns : Columns to project on. get_meta | get_meta(table_names: List[str or Tuple[str, str]], columns: List[str], join_atts: List[str or Tuple[str, str]] = None, mode: CreationMode = CreationMode.NONE, save: bool = True, save_file_name: str = None, batchmode: bool = False, cube: bool = False) -> Dict Method for the whole process of collecting the meta-information for the given tables joined on the given attributes and projected on the given columns. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : List of names of columns, as strings, to project on. join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case save : boolean whether to save the meta-information to file save_file_name : name for the save-file for the meta_information -> not needed if save==False batchmode : whether the meta data is collected in batches or not -> connection to db held open if batch mode mode : see CreationMode-Enum cube : Whether to create the _cube table additionally (only for CreationMode Temporary or Permanent). Returns : dictionary containing the meta-information get_meta_from_file | get_meta_from_file(file_path: str, save: bool = True, save_file_path: str = None, mode: CreationMode = CreationMode.NONE, override: bool = True) -> Dict[int, any] Method for collecting meta data for the information given in a file from QueryParser or at least a file formatted like this. Arguments : file_path : Path to the file. Format has to be the same like the output of QueryParser save : Whether to save the information to file or not. -> It is recommended to do so. save_file_path : Optional path for the save-file. mode : see CreationMode-Enum override : Whether to override an already existing meta_information file. Returns : The solution dict. save_meta | save_meta(meta_dict: Dict, file_name: str = \"meta_information\", mode: str = \"w\") Method for saving the meta-information to file. Arguments : meta_dict : the dictionary containing the meta-information to save file_name : the name (without file-type) for the save-file mode : The mode to open the file. Some common possibilities are 'w', 'w+', 'r', 'a', 'a+'","title":"API Documentation"},{"location":"api-documentation/#main","text":"","title":"main"},{"location":"api-documentation/#estimator","text":"","title":"estimator"},{"location":"api-documentation/#estimatorestimator","text":"","title":"estimator.estimator"},{"location":"api-documentation/#estimator-objects","text":"class Estimator() Class containing the neural network for cardinality estimation. The specifications of the neural network can be changed in 'config.yaml'.","title":"Estimator Objects"},{"location":"api-documentation/#__init__","text":"| __init__(config: Dict[str, Any] = None, config_file_path: str = \"config.yaml\", data: np.ndarray = None, model: Model = None, model_path: str = None, debug: bool = True) Initializer for the Estimator. Configuration options for the neural network are optionally passed via a config dict. It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". Arguments : config : Only used if neither a model or a model_path is passed. if given: It must contain at least the fields \"loss_function\", \"dropout\", \"learning_rate\", \"kernel_initializer\", \"activation_strategy\" and \"layer\". if not given: the config file 'config.yaml' is used for these settings. config_file_path : path for the config-file -> only necessary if no config is given data : Optional parameter for giving the data for training and testing. If given it has to be a Dict with at least \"x\" and \"y\" and optionally \"postgres_estimate\" as keys. The values have to be numpy.ndarray. For key \"x\" it should be the vectorized queries, for key \"y\" the true cardinalities in the same order and for optional key \"postgres_estimate\" the estimates of the postgres optimizer for the query. model : Option to pass a Model which can be used. model_path : Option to pass a path to a saved model in an .h5 file. debug : Boolean whether to print additional information while processing.","title":"__init__"},{"location":"api-documentation/#get_model","text":"| get_model(len_input: int, override: bool = False) -> Model Function for creating the model of the neural network with the information from self.config Arguments : len_input : The size of the input vector. override : Whether an existing model should be overridden. Returns : The model for the neural network with the given properties.","title":"get_model"},{"location":"api-documentation/#load_model","text":"| load_model(model_path: str) Method for loading an already existing model wich was saved to file. Arguments : model_path : Path to the file containing the model to load","title":"load_model"},{"location":"api-documentation/#denormalize","text":"| @staticmethod | denormalize(y, y_min: float, y_max: float) Arguments : y : tensor filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : tensor with denormalized values","title":"denormalize"},{"location":"api-documentation/#denormalize_np","text":"| @staticmethod | denormalize_np(y: np.ndarray, y_min: float, y_max: float) -> np.ndarray Arguments : y : numpy-array filled with values to denormalize y_min : minimum value for y y_max : maximum value for y Returns : numpy-array with denormalized values","title":"denormalize_np"},{"location":"api-documentation/#load_data_file","text":"| load_data_file(file_path: str, override: bool = False) -> Dict[str, np.ndarray] Method for loading the data from file. Arguments : file_path : Path for the file where the data is stored. Has to be a .csv or .npy file. override : Boolean whether to override already existing data. Returns : The data which is set for the Estimator.","title":"load_data_file"},{"location":"api-documentation/#set_data","text":"| set_data(loaded_data: np.ndarray, override: bool = False) Method for setting data and dependent values like max_card and input_length. Arguments : loaded_data : The data loaded from the file. override : Boolean whether to override already existing data.","title":"set_data"},{"location":"api-documentation/#split_data","text":"| split_data(split: float = 0.9) Function to split the data into training- and test-set by a parameterized split value. Arguments : split : Percentage of the data going into training set. (split=0.9 means 90% of data is training set)","title":"split_data"},{"location":"api-documentation/#train","text":"| train(epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1) -> Union[History, History] Method for training the before created Model. Arguments : epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per epoch. shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. validation_split : How much of the data should be taken as validation set -> these are taken from the training data, not the test data, and are reselected for every epoch. Returns : Training history as dict.","title":"train"},{"location":"api-documentation/#test","text":"| test() -> np.ndarray Let the trained neural network predict the test data. Returns : numpy-array containing the normalized predictions of the neural network for the test data","title":"test"},{"location":"api-documentation/#predict","text":"| predict(data: np.ndarray) -> np.ndarray Let the trained neural network predict the given data. Arguments : data : numpy-array containing at least one vectorized query which should be predicted Returns : numpy-array containing the normalized predictions of the neural network for the given data","title":"predict"},{"location":"api-documentation/#run","text":"| run(data_file_path: str = None, epochs: int = 100, verbose: int = 1, shuffle: bool = True, batch_size: int = 32, validation_split: float = 0.1, override_model: bool = False, save_model: bool = True, save_model_file_path: str = \"model\") -> np.ndarray Method for a full run of the Estimator, with training and testing. Arguments : data_file_path : Optional path to saved data file. Only necessary if no data has been set before. epochs : Number of epochs for training. verbose : How much information to print while training. 0 = silent, 1 = progress bar, 2 = one line per epoch. shuffle : Whether to shuffle the training data -> not necessary if split was done by numpy.random.choice() batch_size : Size for the batches -> Smaller batches may be able to train the neural network better (possibly) but enlarge training time, while bigger batches may lead to a less well trained network while training faster. validation_split : How much of the data should be taken as validation set -> these are taken from the training data, not the test data, and are reselected for every epoch. override_model : Whether to override a probably already existing model. save_model : Whether to save the trained model to file. save_model_file_path : When save_model==True this parameter is required to give the path where the model should be saved. Returns : A numpy.ndarray containing the calculated q-error.","title":"run"},{"location":"api-documentation/#save_model","text":"| save_model(filename: str = \"model\") Method for saving the Model to file. Arguments : filename : Name of the file where the model should be stored. (Without file ending. \".h5\" is added to the filename)","title":"save_model"},{"location":"api-documentation/#query_parser","text":"","title":"query_parser"},{"location":"api-documentation/#query_parserquery_parser","text":"","title":"query_parser.query_parser"},{"location":"api-documentation/#queryparser-objects","text":"class QueryParser() Class for the query_parser. This is responsible of reading a given file and return a file containing the aggregated information of this file.","title":"QueryParser Objects"},{"location":"api-documentation/#read_file","text":"| read_file(file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Generic Method for reading the sql statements from a given .sql or a .csv file. Arguments : file_path : Path to the file containing the sql statements. This path has to end with .csv or .sql. No other file types are supported at the moment. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. query_format : The format of the sql query. Look at documentation of QueryFormat for details. :return","title":"read_file"},{"location":"api-documentation/#read_sql_file","text":"| @staticmethod | read_sql_file(file_path: str, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Tuple[Dict, str, str, str] Read the sql statements from given sql file. Arguments : file_path : Path to the file containing the sql statements. query_format : The format of the sql query. Look at documentation of QueryFormat for details. :return","title":"read_sql_file"},{"location":"api-documentation/#read_csv_file","text":"| @staticmethod | read_csv_file(file_path: str, inner_separator: str = \",\", outer_separator: str = \"#\") -> Tuple[Dict, str, str, str] Read the csv formatted sql statements from given file. Arguments : file_path : Path to the file containing the sql statements formatted as csv. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. :return","title":"read_csv_file"},{"location":"api-documentation/#create_solution_dict","text":"| create_solution_dict(command_dict: Dict[str, List[str] or List[Tuple[str, str]]], file_type: str, inner_separator: str) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for building the solution dict. Arguments : command_dict : Dict with a alphabetical sorted string of the joining tables as key and a list of where clauses as string if the file type is sql or a list of tuples containing the join-attribute-string in first and the selection-attribute-string in second place. file_type : String with 'csv'/'tsv' or 'sql' which tells the file type of the read file. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. :return The solution dict containing 'table_names', 'join_attributes' and 'selection_attributes'.","title":"create_solution_dict"},{"location":"api-documentation/#table_name_unpacker","text":"| @staticmethod | table_name_unpacker(from_string: str, separator: str = \",\") -> List[Tuple[str, str]] Takes the sorted string of the from clause and extracts the tables with their aliases. Arguments : from_string : Alphabetical ordered string containing all tables to join, separated by the separator. separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : List of tuples where the first element of the tuple is the table name and the second one is the alias.","title":"table_name_unpacker"},{"location":"api-documentation/#sql_attribute_unpacker","text":"| sql_attribute_unpacker(where_string_list: List[str]) -> Tuple[List[str], List[str]] Unpack the attribute strings from sql-file into sets containing the attributes. Arguments : where_string_list : A list of strings from the where clauses. These have to be separated into join- and selection-attributes. Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second place.","title":"sql_attribute_unpacker"},{"location":"api-documentation/#csv_attribute_unpacker","text":"| csv_attribute_unpacker(attribute_tuples: List[Tuple[str, str]], separator: str = \",\") -> Tuple[List[str], List[str]] Unpack the attribute strings from csv-file into sets containing the attributes. Arguments : attribute_tuples : A list of tuples of strings where the first string is the string for all join-attributes, while the second string contains all selection-attributes. separator : The column separator used in the file. You can use '\\t' for .tsv files. Returns : A tuple containing the list of join-attributes in first and the list of selection-attributes in second place.","title":"csv_attribute_unpacker"},{"location":"api-documentation/#save_solution_dict","text":"| @staticmethod | save_solution_dict(solution_dict: Dict[int, Dict[str, List[str or Tuple[str, str]]]], save_file_path: str = \"solution_dict\") Save the solution to file with specified filename. Arguments : solution_dict : The dict containing the data to save. save_file_path : The path for the file in which the data should be saved. The .yaml ending is added automatically.","title":"save_solution_dict"},{"location":"api-documentation/#run_1","text":"| run(file_path: str, save_file_path: str, inner_separator: str = None, outer_separator: str = None, query_format: QueryFormat = QueryFormat.CROSS_PRODUCT) -> Dict[int, Dict[str, List[str or Tuple[str, str]]]] Method for the whole parsing process. Arguments : file_path : The file to read in which the sql-statements are saved. save_file_path : The path where to save the results. inner_separator : The column separator used in the file. You can use '\\t' for .tsv files. -> See documentation for details. outer_separator : The block separator used in the file. -> See documentation for details. query_format : The indicator for the format of the .sql query-file. If the given file is not .sql than this is not used. Returns :","title":"run"},{"location":"api-documentation/#database_connector","text":"","title":"database_connector"},{"location":"api-documentation/#database_connectordatabase_connector","text":"","title":"database_connector.database_connector"},{"location":"api-documentation/#database-objects","text":"class Database(Enum) Enum for the different supported databases. If you use MySQL, then use MARIADB as value here.","title":"Database Objects"},{"location":"api-documentation/#databaseconnector-objects","text":"class DatabaseConnector() Class for DatabaseConnector.","title":"DatabaseConnector Objects"},{"location":"api-documentation/#__init___1","text":"| __init__(database: Database, debug: bool = True) Initializer for the DatabaseConnector Arguments : debug : boolean whether to print additional information while processing","title":"__init__"},{"location":"api-documentation/#connect","text":"| connect(config: Dict = None, config_file_path: str = None, sqlite_file_path: str = None) Wrapper method for connecting to the selected database. Arguments : config : if given: It has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 5432 if not given) for PostgreSQL or it has to be a dictionary with at least database, user and password and optionally host and port (default to host: localhost, port: 3306 if not given) for MariaDB. if not given: The config file path is needed and used for these settings. config_file_path : Path to the config file for PostgreSQL or MariaDB. sqlite_file_path : Path to the SQLite database file.","title":"connect"},{"location":"api-documentation/#connect_to_postgres","text":"| connect_to_postgres(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : config : if given: it has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 5432 if not given) if not given: the config file 'config.yaml' is used for these settings config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a .yaml/.yml file","title":"connect_to_postgres"},{"location":"api-documentation/#connect_to_mariadb","text":"| connect_to_mariadb(config: Dict = None, config_file_path: str = \"config.yaml\") Connect to the postgres database with the given config. Arguments : config : if given: it has to be a dictionary with at least db_name, user and password and optionally host and port (default to host: localhost, port: 3306 if not given) if not given: the config file 'config.yaml' is used for these settings config_file_path : path for the config-file -> only necessary if no config is given; needs to point on a .yaml/.yml file","title":"connect_to_mariadb"},{"location":"api-documentation/#connect_to_sqlite","text":"| connect_to_sqlite(database_file_path: str) Open connection to a sqlite database. Arguments : database_file_path : The path to the sqlite database file.","title":"connect_to_sqlite"},{"location":"api-documentation/#close_database_connection","text":"| close_database_connection() close the connection to the database Returns : void","title":"close_database_connection"},{"location":"api-documentation/#execute","text":"| execute(sql_string: str) Method for executing a SQL-Query. Arguments : sql_string : The SQL-Query to execute","title":"execute"},{"location":"api-documentation/#fetchall","text":"| fetchall() Wrapper for fetchall method.","title":"fetchall"},{"location":"api-documentation/#fetchone","text":"| fetchone() Wrapper for fetchone method.","title":"fetchone"},{"location":"api-documentation/#vectorizer","text":"","title":"vectorizer"},{"location":"api-documentation/#vectorizervectorizer","text":"","title":"vectorizer.vectorizer"},{"location":"api-documentation/#vectorizer-objects","text":"class Vectorizer() Constructs a vector consisting of operator code and normalized value for each predicate in the sql query set with set_query method.","title":"Vectorizer Objects"},{"location":"api-documentation/#__init___2","text":"| __init__() Intitialises the Vectorizer object by defining available operators.","title":"__init__"},{"location":"api-documentation/#add_queries_with_cardinalities","text":"| add_queries_with_cardinalities(queries_with_cardinalities_path: str) Reads CSV file with format (querySetID;query;encodings;max_card;min_max_step;estimated_cardinality;true_cardinality) whereas min_max_step is an array of the format [[1, 2, 1], [1, 113, 1], [1878, 2115, 1]] sorted by lexicographic order of corresponding predicates and encodings is an empty array if only integer values are processed. For a querySetID all predicates are collected and sorted in lexicographical order to provide correct indices (e.g. in encodings & min_max_value) for a given predicate. Read queries are added to the list of vectorisation tasks. Arguments : queries_with_cardinalities_path : path to a CSV file containing all queries and their estimated and true cardinalities","title":"add_queries_with_cardinalities"},{"location":"api-documentation/#vectorize","text":"| vectorize() -> List[np.array] Vectorizes all vectorization tasks added. Returns : List of np.array vectors whereas each row contains the vectorized query and appended maximal, estimated and true cardinality (in this order)","title":"vectorize"},{"location":"api-documentation/#save","text":"| save(base_path: str, result_folder: str, base_filename: str, filetypes: str) Stores the SQL query and corresponding vector at given path as NPY and TXT file. Arguments : base_path : path to a directory for saving result_folder : name of folder to create for storing multiple files. This argument is seperated from base_path to empathize the need for an extra folder, since multiple files are saved. filename : filename without filetype. querySetID is appended for differentiation filetypes : string of file types must contain \"csv\" or \"npy\"","title":"save"},{"location":"api-documentation/#vectorize_query_original","text":"vectorize_query_original(query: str, min_max: Dict[str, Tuple[int, int, int]], encoders: List[Dict[str, int]]) -> np.array Copy-pasted method of the original implementation for testing purposes; Only added Join detection Arguments : query : the query to vectorize min_max : dictionary of all min, max, step values for each predicate encoders : dictionary, which maps predicates to encoders Returns : the normalized vector without cardinalities","title":"vectorize_query_original"},{"location":"api-documentation/#vectorizer_tests","text":"vectorizer_tests() Test method to compare the original implementation with jupyter notebook output (truth) or with the Vectorizer implementation. Succeeds if no assertion throws an error.","title":"vectorizer_tests"},{"location":"api-documentation/#query_communicator","text":"","title":"query_communicator"},{"location":"api-documentation/#query_communicatorsql_generator","text":"","title":"query_communicator.sql_generator"},{"location":"api-documentation/#query_communicatorsql_generatorsql_generator","text":"","title":"query_communicator.sql_generator.sql_generator"},{"location":"api-documentation/#sqlgenerator-objects","text":"class SQLGenerator() Class for generating SQL queries. Uses Meta Information from MetaCollector Step before.","title":"SQLGenerator Objects"},{"location":"api-documentation/#__init___3","text":"| __init__(config: str = None, debug: bool = False) Initializer of the SQL Generator. Reads the needed parameters from the meta-information.yaml generated from the MetaCollector which was possibly executed before. It's also possible to pass an own meta.yaml file to skip the metaCollector Step. Arguments : config : Config file which contains columns with their datatypes, tables, join_attributes, encodings, maximum cardinality min and max values of the columns with the step size. If None: meta-information.yaml generated from MetaCollector is used","title":"__init__"},{"location":"api-documentation/#write_sql","text":"| @staticmethod | write_sql(queries: List[Tuple[int, str]], file: str) Function for writing a human readable sql file with the generated queries Arguments : queries : list of queries, containing querySetID and query as tuple file : file name for sql file Returns :","title":"write_sql"},{"location":"api-documentation/#random_operator_value","text":"| @staticmethod | random_operator_value(range: List[int], val_type: str, encoding: dict = None) -> Tuple[str, str or int] Function for random operator and value creation from a list of allowed operators and a range of values Arguments : range : Min_max Value range and stepsize of a given column from meta_information val_type : Type of column, can be either integer or enumeration. when enumeration, then an encoding dict has to be given encoding : Encoding of the possible values for the column Returns : Tuple consisting of operator and value as string or int","title":"random_operator_value"},{"location":"api-documentation/#generate_queries","text":"| generate_queries(qnumber: int = 10, save_readable: str = 'queries') Generates given number of SQL Queries with given meta-information. Function, which models the hole process of query generation Arguments : self : save_readable : Saves the generated SQL queries into a human friendly readable .sql file qnumber : Number of generated queries per meta-entry Returns : list with queries as String","title":"generate_queries"},{"location":"api-documentation/#query_communicatorquery_communicator","text":"","title":"query_communicator.query_communicator"},{"location":"api-documentation/#querycommunicator-objects","text":"class QueryCommunicator() Class for oberserving the generation and evaluation of queries, in order to have nullqueryfree set of queries if needed. Manages the communication between Evaluator and SQL Generator to get the required amount of queries if possible. The SQL_Generator itself is not able to find nullqueries, that are caused by a valid combination of attributes, which just don't match any data of the database. Vice Versa, the Evaluator is not able to generate new queries, if there are nullqueries.","title":"QueryCommunicator Objects"},{"location":"api-documentation/#get_queries","text":"| get_queries(database_connector: DatabaseConnector, save_file_path: str, query_number: int) Function for generating queries and their cardinalities if nullqueries are allowed. Saves generated queries in ../assets/queries_with_cardinalities.csv Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns :","title":"get_queries"},{"location":"api-documentation/#get_nullfree_queries","text":"| get_nullfree_queries(query_number: int, save_file_path: str, database_connector: DatabaseConnector) Function that generates given number queries and their cardinalities which are not zero. There will be less queries then requested, if unavoidable. Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities database_connector : Handles the database connection to the desired database. Returns : list of remained Queries","title":"get_nullfree_queries"},{"location":"api-documentation/#reduce_queries","text":"| @staticmethod | reduce_queries(query_number: int, save_file_path: str) -> List Reduces genrated queries to the requested number of queries Arguments : query_number : number of queries to generate save_file_path : path to save the finished queries with their cardinalities Returns : DataFrame with reduced query sets","title":"reduce_queries"},{"location":"api-documentation/#write_queries","text":"| @staticmethod | write_queries(queries: List, save_file_path: str = 'assets/reduced_queries_with_cardinalities.csv') function for writing the csv file with the reduced queries Arguments : queries : list of queries to write in a csv file save_file_path : file path, where to save the file Returns :","title":"write_queries"},{"location":"api-documentation/#produce_queries","text":"| produce_queries(database_connector: DatabaseConnector, query_number: int = 10, nullqueries: bool = False, save_file_path: str = 'assets/reduced_queries_with_cardinalities.csv') Main function to produce the queries and return the correct csv file, depending if nullqueries are wanted or not Arguments : save_file_path : Path to save the finished query file nullqueries : decide whether to generate nullqueries or not, default: no nullqueries query_number : count of queries that are generated per meta file entry database_connector : Connector for the database connection, depending on the database system you are using Returns :","title":"produce_queries"},{"location":"api-documentation/#query_communicatordatabase_evaluator","text":"","title":"query_communicator.database_evaluator"},{"location":"api-documentation/#query_communicatordatabase_evaluatordatabase_evaluator","text":"","title":"query_communicator.database_evaluator.database_evaluator"},{"location":"api-documentation/#databaseevaluator-objects","text":"class DatabaseEvaluator() Class for DatabaseEvaluator. Using psycopg2 to establish a connection to the postgres database. Evaluate true and estimated cardinalities from q given query list and save them.","title":"DatabaseEvaluator Objects"},{"location":"api-documentation/#__init___4","text":"| __init__(database_connector: DatabaseConnector, debug: bool = True, input_file_name: str = 'queries.csv') Initializer for the DatabaseEvaluator Configuration options for the database are optionally passed via a config dict. It must contain at least the dbname, the username and the password. Additionally the host and the port can be given if there not default (host: localhost, port: 5432). Arguments : debug : boolean whether to print additional information while processing input_file_name : name of the file used for the sql query import, have to be .csv or .sql and located in the asset folder database_connector : Handles the database connection to the desired database.","title":"__init__"},{"location":"api-documentation/#import_sql_queries","text":"| import_sql_queries(path) load the queries from sql or csv file, wich is provided by the sql_generator submodule Arguments : path : path to the file with the given queries (per default in asset folder), relative to the database_evaluator folder Returns : void","title":"import_sql_queries"},{"location":"api-documentation/#generate_explain_queries","text":"| generate_explain_queries() generate EXPLAIN sql statements for cardinality estimation Returns : void","title":"generate_explain_queries"},{"location":"api-documentation/#get_true_cardinalities","text":"| get_true_cardinalities() execute the given queries against the database and calculate the true cardinality from each query Returns : void","title":"get_true_cardinalities"},{"location":"api-documentation/#get_estimated_cardinalities","text":"| get_estimated_cardinalities() execute the adapted queries against the database and calculate the postgres cardinality estimation for each query Returns : void","title":"get_estimated_cardinalities"},{"location":"api-documentation/#save_cardinalities","text":"| save_cardinalities(save_readable: Tuple[bool, str] = (True, 'assets/queries_with_cardinalities.txt'), save_file_path: str = 'assets/queries_with_cardinalities.csv', eliminate_null_queries: bool = True) execute the adapted queries against the database and calculate the postgres cardinality estimation for each query Arguments : eliminate_null_queries : if True only queries with true cardinality > 0 will be saved save_readable : if True: save queries and corresponing cardinalities human readable in an separate text file, per default as assets/queries_with_cardinalities.txt save_file_path : path to save the finished queries with their cardinalities Returns : void","title":"save_cardinalities"},{"location":"api-documentation/#get_cardinalities","text":"| get_cardinalities(eliminate_null_queries: bool = True, save_file_path: str = 'assets/queries_with_cardinalities.csv') function that manage the whole process of cardinality estimation/calculation Arguments : eliminate_null_queries : if True only queries with true cardinality > 0 will be saved save_file_path : path to save the finished queries with their cardinalities Returns : void","title":"get_cardinalities"},{"location":"api-documentation/#meta_collector","text":"","title":"meta_collector"},{"location":"api-documentation/#meta_collectormeta_collector","text":"","title":"meta_collector.meta_collector"},{"location":"api-documentation/#creationmode-objects","text":"class CreationMode(Enum) Enum for the different possibilities to use the MetaCollector. 0 -> don't create table, 1 -> create temporary table, 2 -> create permanent table","title":"CreationMode Objects"},{"location":"api-documentation/#metacollector-objects","text":"class MetaCollector() Class for MetaCollector.","title":"MetaCollector Objects"},{"location":"api-documentation/#__init___5","text":"| __init__(database_connector: DatabaseConnector, debug: bool = True) Initializer for the MetaCollector Arguments : database_connector : The connector to the used database. debug : boolean whether to print additional information while processing","title":"__init__"},{"location":"api-documentation/#get_columns_data_postgres","text":"| get_columns_data_postgres(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for PostgreSQL and MariaDB. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the data-type of the column, the tuple containing min value, max value and step size, the encodings if datatype is not int and the alternative column-name if the original one occurs more than once.","title":"get_columns_data_postgres"},{"location":"api-documentation/#get_columns_data_sqlite","text":"| get_columns_data_sqlite(table_names: List[str or Tuple[str, str]], columns: List[str]) -> List[Tuple[str, str, str, Tuple[int, int, int], Dict[str, LabelEncoder], str]] Get column-name and datatype for the requested columns of the corresponding tables for SQLite. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : Columns to project on. Returns : A list containing the name of the column, the table alias (if existent, else the table-name), the data-type of the column, the tuple containing min value, max value and step size, the encodings if datatype is not int and the alternative column-name if the original one occurs more than once.","title":"get_columns_data_sqlite"},{"location":"api-documentation/#collect_min_max_step","text":"| collect_min_max_step(tablename: str, column: Tuple[str, str]) -> (Tuple[int, int, int], Dict) After collecting the datatype information for the columns this function returns the min and max values for the meta-table and the encoders. Arguments : tablename : String containing the name of the table where to find the column column : a tuple containing the name and the datatype for the column, each as string Returns : first: dictionary with the attribute-name as key and a tuple containing min-value, max-value and step-size (all as int) as value second: a dictionary of the not integer encoders with key attribute-name and value the encoder","title":"collect_min_max_step"},{"location":"api-documentation/#get_max_card","text":"| get_max_card(table_names: List[str or Tuple[str, str]], join_atts: List[str or Tuple[str, str]] = None) -> int Get the size of the join-table without any selections, the so called max-card. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case Returns :","title":"get_max_card"},{"location":"api-documentation/#setup_view","text":"| setup_view(table_names: List[str or Tuple[str, str]], columns_types: List[Tuple], join_atts: List[str or Tuple[str, str]] = None, cube: bool = False, mode: CreationMode = CreationMode.NONE) -> (List[Tuple[str, str]], int) Create the tables tmpview and if cube==True also tmpview_cube containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns_types : columns to project on join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case cube : boolean whether to create the *_cube table, too mode : see CreationMode-Enum Returns : first: a list of tuples containing the name and the datatype for the columns, each as string second: the maximal cardinality as integer","title":"setup_view"},{"location":"api-documentation/#setup_cube_view","text":"| setup_cube_view(new_table_name: str, columns) Create the table tmpview_cube if cube==True containing the metadata for the given tables joined on the attributes and projected on the columns. Arguments : new_table_name : The name of the before created table. columns : Columns to project on.","title":"setup_cube_view"},{"location":"api-documentation/#get_meta","text":"| get_meta(table_names: List[str or Tuple[str, str]], columns: List[str], join_atts: List[str or Tuple[str, str]] = None, mode: CreationMode = CreationMode.NONE, save: bool = True, save_file_name: str = None, batchmode: bool = False, cube: bool = False) -> Dict Method for the whole process of collecting the meta-information for the given tables joined on the given attributes and projected on the given columns. Arguments : table_names : List of names of tables, as strings or tuples containing table-name in first and alias in second place, to join. columns : List of names of columns, as strings, to project on. join_atts : List of attributes, as strings or tuples containing the two attributes to join with '=', to join the tables on. -> is optional, because there is no join if there is only one table and so there would be no join-attribute needed in that case save : boolean whether to save the meta-information to file save_file_name : name for the save-file for the meta_information -> not needed if save==False batchmode : whether the meta data is collected in batches or not -> connection to db held open if batch mode mode : see CreationMode-Enum cube : Whether to create the _cube table additionally (only for CreationMode Temporary or Permanent). Returns : dictionary containing the meta-information","title":"get_meta"},{"location":"api-documentation/#get_meta_from_file","text":"| get_meta_from_file(file_path: str, save: bool = True, save_file_path: str = None, mode: CreationMode = CreationMode.NONE, override: bool = True) -> Dict[int, any] Method for collecting meta data for the information given in a file from QueryParser or at least a file formatted like this. Arguments : file_path : Path to the file. Format has to be the same like the output of QueryParser save : Whether to save the information to file or not. -> It is recommended to do so. save_file_path : Optional path for the save-file. mode : see CreationMode-Enum override : Whether to override an already existing meta_information file. Returns : The solution dict.","title":"get_meta_from_file"},{"location":"api-documentation/#save_meta","text":"| save_meta(meta_dict: Dict, file_name: str = \"meta_information\", mode: str = \"w\") Method for saving the meta-information to file. Arguments : meta_dict : the dictionary containing the meta-information to save file_name : the name (without file-type) for the save-file mode : The mode to open the file. Some common possibilities are 'w', 'w+', 'r', 'a', 'a+'","title":"save_meta"}]}